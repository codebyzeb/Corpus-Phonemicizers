{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training the CHILDES Tokenizer\n",
    "\n",
    "Using the phonemes in our CHILDES dataset, we train a tokenizer that just splits according to whitespace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zebulongoriely/Documents/UniDocs/PHD/research/projects/CHILDES_processor/env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "None of PyTorch, TensorFlow >= 2.0, or Flax have been found. Models won't be available and only tokenizers, configuration and file/data utilities can be used.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from datasets import load_dataset, get_dataset_config_names\n",
    "from tokenizers import Tokenizer, models, normalizers, pre_tokenizers, trainers, processors, decoders\n",
    "from transformers import GPT2TokenizerFast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nz/6tzh0bsj2txd1cz18gpcms_c0000gn/T/ipykernel_54436/769076702.py:1: DtypeWarning: Columns (4,7,8,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  phoible = pd.read_csv('../../../data/phoible.csv')\n"
     ]
    }
   ],
   "source": [
    "phoible = pd.read_csv('../../../data/phoible.csv')\n",
    "phoible_phonemes = phoible.Phoneme.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_COUNT = 10\n",
    "\n",
    "def build_vocabulary(datasets, column='phonemized_utterance', allow_non_phoible=False):\n",
    "\n",
    "    vocab = {'UNK' : 0, 'PAD' : 1, 'WORD_BOUNDARY' : 2, 'UTT_BOUNDARY' : 3}\n",
    "    unk_tokens = []\n",
    "    token_counts = {}\n",
    "    for dataset in datasets:\n",
    "        for line in dataset[column]:\n",
    "            tokens = line.strip().split()\n",
    "            for token in tokens:\n",
    "                if token not in token_counts:\n",
    "                    token_counts[token] = 0\n",
    "                token_counts[token] += 1\n",
    "        \n",
    "    # Add tokens to vocab if they are not in phoible and have a count greater than MIN_COUNT\n",
    "    for token, count in token_counts.items():\n",
    "        if count > MIN_COUNT and token not in vocab:\n",
    "            if token not in phoible_phonemes and not allow_non_phoible:\n",
    "                unk_tokens.append(token)\n",
    "            else:\n",
    "                vocab[token] = len(vocab)\n",
    "\n",
    "    print('Tokens not found in phoible: ', {token: token_counts[token] for token in unk_tokens})\n",
    "    print('Vocab: ', vocab)\n",
    "    print('Vocab size: ', len(vocab))\n",
    "    return vocab\n",
    "\n",
    "def build_phoneme_tokenizer(vocab):\n",
    "\n",
    "    tokenizer = Tokenizer(models.WordLevel(vocab=vocab, unk_token='UNK'))\n",
    "    tokenizer.normalizer = normalizers.Sequence([normalizers.Strip()]) \n",
    "    tokenizer.add_special_tokens([\"UNK\", \"PAD\"])\n",
    "    tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "    tokenizer.post_processor = processors.TemplateProcessing(\n",
    "        single=\"UTT_BOUNDARY $A\",\n",
    "        pair=\"UTT_BOUNDARY $A UTT_BOUNDARY $B:1\",\n",
    "        special_tokens=[(\"UTT_BOUNDARY\", tokenizer.token_to_id(\"UTT_BOUNDARY\"))],\n",
    "    )\n",
    "\n",
    "    wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer, bos_token='UTT_BOUNDARY', eos_token='UTT_BOUNDARY', pad_token='PAD', unk_token='UNK')\n",
    "    return wrapped_tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Tokenizer for each language in CHILDES\n",
    "\n",
    "We create a unique tokenizer for each language, to keep the vocabulary size appropriate for each language. For most languages we remove any tokens not found in Phoible. We do not do this for Mandarin or Cantonese as for these languages we merge the tone marker and preceding vowel into one phoneme, whereas Phoible treats tone markers as independent symbols."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Languages: ['English', 'EnglishUK', 'French', 'German', 'Spanish', 'Dutch', 'Mandarin', 'Japanese', 'Cantonese', 'Estonian', 'Croatian', 'Danish', 'Basque', 'Hungarian', 'Turkish', 'Farsi', 'Icelandic', 'Indonesian', 'Irish', 'Welsh', 'Korean', 'Swedish', 'Norwegian', 'Quechua', 'Catalan', 'Italian', 'PortuguesePt', 'PortugueseBr', 'Romanian']\n"
     ]
    }
   ],
   "source": [
    "languages = get_dataset_config_names('transformersegmentation/CHILDES')\n",
    "print('Languages:', languages)\n",
    "datasets = [load_dataset('transformersegmentation/CHILDES', language, split='train') for language in languages]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Trainking tokenizer for English...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 's': 4, 'iː': 5, 'ð': 6, 'ɛ': 7, 'ɹ': 8, 'z': 9, 'ʌ': 10, 'f': 11, 'eɪ': 12, 'w': 13, 'ɪ': 14, 'ɡ': 15, 'l': 16, 'æ': 17, 'ɑ': 18, 'h': 19, 'ə': 20, 'ʊ': 21, 'k': 22, 'p': 23, 'uː': 24, 'b': 25, 'i': 26, 't': 27, 'aɪ': 28, 'θ': 29, 'ŋ': 30, 'j': 31, 'ɔ': 32, 'm': 33, 'ɔɪ': 34, 'n': 35, 'd': 36, 'oʊ': 37, 'aʊ': 38, 'v': 39, 'ɜː': 40, 't̠ʃ': 41, 'd̠ʒ': 42, 'ʃ': 43, 'iə': 44, 'ʒ': 45, 'ɑ̃': 46, 'r': 47, 'x': 48}\n",
      "Vocab size:  49\n",
      "Tokenizer for English pushed to the hub.\n",
      "\n",
      "Trainking tokenizer for EnglishUK...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'eɪ': 4, 't̠ʃ': 5, 'w': 6, 'ɒ': 7, 't': 8, 'd': 9, 'ʌ': 10, 'z': 11, 'ð': 12, 'a': 13, 'm': 14, 'iː': 15, 'n': 16, 'ɛ': 17, 'k': 18, 's': 19, 'ɪ': 20, 'ɡ': 21, 'ʊ': 22, 'ɑː': 23, 'ɔː': 24, 'l': 25, 'ə': 26, 'ɹ': 27, 'i': 28, 'əʊ': 29, 'uː': 30, 'j': 31, 'h': 32, 'iə': 33, 'ɔɪ': 34, 'v': 35, 'aɪ': 36, 'f': 37, 'ɜː': 38, 'b': 39, 'p': 40, 'd̠ʒ': 41, 'ɐ': 42, 'eə': 43, 'ʃ': 44, 'θ': 45, 'ŋ': 46, 'aʊ': 47, 'ʊə': 48, 'n̩': 49, 'ʒ': 50, 'r': 51, 'ɑ̃': 52, 'aː': 53}\n",
      "Vocab size:  54\n",
      "Tokenizer for EnglishUK pushed to the hub.\n",
      "\n",
      "Trainking tokenizer for French...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 't': 4, 'y': 5, 'v': 6, 'j': 7, 'ɛ̃': 8, 'w': 9, 'a': 10, 'ʁ': 11, 'd': 12, 'e': 13, 'ʒ': 14, 'm': 15, 'ɔ̃': 16, 'p': 17, 'ɛ': 18, 'f': 19, 'ɔ': 20, 'ɑ̃': 21, 's': 22, 'z': 23, 'l': 24, 'ə': 25, 'b': 26, 'k': 27, 'u': 28, 'o': 29, 'ʃ': 30, 'ɡ': 31, 'i': 32, 'n': 33, 'œ̃': 34, 'ø': 35, 'œ': 36, 'oː': 37, 'yː': 38, 'ɲ': 39, 'aː': 40, 't̠ʃ': 41}\n",
      "Vocab size:  42\n",
      "Tokenizer for French pushed to the hub.\n",
      "\n",
      "Trainking tokenizer for German...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'h': 4, 'a': 5, 'l': 6, 'oː': 7, 'j': 8, 'aː': 9, 'ə': 10, 'm': 11, 'd': 12, 's': 13, 't': 14, 'iː': 15, 'n': 16, 'z': 17, 'ɛ': 18, 'ts': 19, 'ɪ': 20, 'eː': 21, 'ʀ': 22, 'aʊ': 23, 'ɡ': 24, 'ŋ': 25, 'ʊ': 26, 'v': 27, 'aɪ': 28, 'uː': 29, 'k': 30, 'ç': 31, 'b': 32, 'ɐ': 33, 'ʃ': 34, 'ɔ': 35, 'x': 36, 'œ': 37, 'f': 38, 'p': 39, 'ɛɪ': 40, 'ʏ': 41, 'yː': 42, 'y': 43, 'ɛː': 44, 'pf': 45, 'øː': 46, 't̠ʃ': 47, 'd̠ʒ': 48, 'ʒ': 49, 'ɔ̃': 50, 'ã': 51, 'w': 52, 'i': 53}\n",
      "Vocab size:  54\n",
      "Tokenizer for German pushed to the hub.\n",
      "\n",
      "Trainking tokenizer for Spanish...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'k': 4, 'o': 5, 'm': 6, 'e': 7, 's': 8, 'i': 9, 'ɾ': 10, 'n': 11, 'a': 12, 'u': 13, 'j': 14, 'p': 15, 'ð': 16, 'l': 17, 't': 18, 'β': 19, 'θ': 20, 'ɡ': 21, 'w': 22, 'ʎ': 23, 'f': 24, 'd': 25, 'ɣ': 26, 'r': 27, 'x': 28, 'ʝ': 29, 'ɲ': 30, 'b': 31, 'oɪ': 32, 'aɪ': 33, 'ɛ': 34, 'ŋ': 35, 't̠ʃ': 36, 'eʊ': 37, 'eɪ': 38, 'aʊ': 39, 'd̠ʒ': 40, 'pː': 41, 'ʃ': 42, 'ts': 43}\n",
      "Vocab size:  44\n",
      "Tokenizer for Spanish pushed to the hub.\n",
      "\n",
      "Trainking tokenizer for Dutch...\n",
      "Tokens not found in phoible:  {'ʌʊ': 29128, 'yʊ': 655}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'j': 4, 'aː': 5, 'h': 6, 'oː': 7, 'r': 8, 'd': 9, 'i': 10, 'ɛ': 11, 'p': 12, 'ɪ': 13, 'k': 14, 'ɑ': 15, 'l': 16, 'eː': 17, 'n': 18, 's': 19, 'v': 20, 'ə': 21, 'ɛɪ': 22, 'ʋ': 23, 'z': 24, 't': 25, 'm': 26, 'ɣ': 27, 'ɪː': 28, 'ɵ': 29, 'ɔ': 30, 'x': 31, 'u': 32, 'f': 33, 'ŋ': 34, 'øː': 35, 'b': 36, 'ɔː': 37, 'y': 38, 'œy': 39, 'tʲ': 40, 'w': 41, 'ɾ': 42, 'ʃ': 43, 'ɛː': 44, 't̠ʃ': 45, 'ɲ': 46, 'ʒ': 47, 'eʊ': 48, 'ɡ': 49, 'd̠ʒ': 50, 'iː': 51, 'a': 52}\n",
      "Vocab size:  53\n",
      "Tokenizer for Dutch pushed to the hub.\n",
      "\n",
      "Trainking tokenizer for Mandarin...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'au̯': 4, 'n': 5, 'a˥˩': 6, 'ʂ': 7, 'ɻ̩˥˩': 8, 'ə˧˥': 9, 'm': 10, 'ɤ': 11, 'p': 12, 'j': 13, 'e˧˥': 14, 'kʰ': 15, 'k': 16, 'ɤ˥˩': 17, 'w': 18, 'o˥': 19, 'a˥': 20, 'ʈʂʰ': 21, 'ə˥': 22, 'ŋ': 23, 't': 24, 'ʊ˥': 25, 'ɕ': 26, 'i': 27, 'a': 28, 'l': 29, 'au̯˧˩˧': 30, 'x': 31, 'u˧˩˧': 32, 'i˥': 33, 'ei̯˧˩˧': 34, 'pʰ': 35, 'i˧˥': 36, 'ai̯˧˥': 37, 'ou̯˧˩˧': 38, 'u˧˥': 39, 'ɤ˧˥': 40, 'o˧˩˧': 41, 'tɕ': 42, 'au̯˥˩': 43, 'ts': 44, 'ə˧˩˧': 45, 'ɤ˥': 46, 'ei̯˧˥': 47, 'ʊ˧˥': 48, 'i˧˩˧': 49, 'ʈʂ': 50, 'ɻ̩˧˩˧': 51, 'ei̯˥˩': 52, 's': 53, 'u˥˩': 54, 'ɹ̩': 55, 'ai̯˥': 56, 'u˥': 57, 'tɕʰ': 58, 'a˧˩˧': 59, 'a˧˥': 60, 'ai̯˥˩': 61, 'ɛ˥˩': 62, 'f': 63, 'i˥˩': 64, 'y˥˩': 65, 'ou̯˥˩': 66, 'e˥': 67, 'tʰ': 68, 'ɹ̩˥˩': 69, 'ɛ˧˥': 70, 'au̯˥': 71, 'ou̯˧˥': 72, 'ɻ': 73, 'e˧˩˧': 74, 'ɛ˥': 75, 'ɻ̩˥': 76, 'ɥ': 77, 'ɹ̩˧˩˧': 78, 'au̯˧˥': 79, 'ai̯˧˩˧': 80, 'ou̯˥': 81, 'o˥˩': 82, 'ɛ˧˩˧': 83, 'ʊ˧˩˧': 84, 'ɔ˥': 85, 'tsʰ': 86, 'ei̯': 87, 'ə˥˩': 88, 'o': 89, 'ʊ˥˩': 90, 'ou̯': 91, 'ɤ˧˩˧': 92, 'o˧˥': 93, 'ei̯˥': 94, 'e˥˩': 95, 'ɚ˧˩˧': 96, 'y˥': 97, 'ɚ˥˩': 98, 'y˧˥': 99, 'ɻ̩': 100, 'y˧˩˧': 101, 'ɹ̩˥': 102, 'ɻ̩˧˥': 103, 'u': 104, 'ai̯': 105, 'ʊ': 106, 'e': 107, 'ɚ˧˥': 108, 'ə': 109, 'ɔ˥˩': 110, 'ɹ̩˧˥': 111, 'ɛ': 112, 'y': 113, 'm˧˥': 114}\n",
      "Vocab size:  115\n",
      "Tokenizer for Mandarin pushed to the hub.\n",
      "\n",
      "Trainking tokenizer for Japanese...\n",
      "Tokens not found in phoible:  {'g': 58660, 't͡s': 22642, 'gʲ': 1102}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'b': 4, 'aː': 5, 'ɯː': 6, 'n': 7, 'ɯ': 8, 'j': 9, 'o': 10, 'i': 11, 'ʃ': 12, 'd': 13, 'r': 14, 'e': 15, 'a': 16, 'k': 17, 'oː': 18, 'h': 19, 'd̠ʒ': 20, 't': 21, 'z': 22, 'eː': 23, 'm': 24, 't̠ʃ': 25, 'w': 26, 'pʲ': 27, 'p': 28, 'ɲ': 29, 'ɸ': 30, 'rʲ': 31, 'kʲ': 32, 'ç': 33, 'bʲ': 34, 'mʲ': 35}\n",
      "Vocab size:  36\n",
      "Tokenizer for Japanese pushed to the hub.\n",
      "\n",
      "Trainking tokenizer for Cantonese...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'aː˧': 4, 't': 5, 'ɐ˥': 6, 'k': 7, 'l': 8, 'j': 9, 'ʊ˥': 10, 'aː˧˩̰': 11, 'ɛː˥': 12, 'n': 13, 'ei˩˧': 14, 'w': 15, 'aː˨': 16, 'ɐi˧˥': 17, 'm̩˧˥': 18, 'm': 19, 'ou˥': 20, 'aː˧˥': 21, 'ei˥': 22, 'iː˧': 23, 'ts': 24, 'ɔː˧˥': 25, 'tʰ': 26, 'iː˥': 27, 'f': 28, 'aːĭ˧': 29, 'ɐ˨': 30, 'p': 31, 'h': 32, 'ɵy˧': 33, 'aː˥': 34, 'ou˨': 35, 'ɔː˧': 36, 'ɐi˧˩̰': 37, 'uː˧': 38, 'ŋ': 39, 's': 40, 'ɔːĭ˥': 41, 'ɐu˨': 42, 'iː˨': 43, 'ei˧˥': 44, 'ɐi˨': 45, 'ʊ˧˩̰': 46, 'ʊ˨': 47, 'aː˩˧': 48, 'aːĭ˧˥': 49, 'ɔː˨': 50, 'ɛː˩˧': 51, 'ɪ˨': 52, 'iːŭ˧': 53, 'ɛː˧˩̰': 54, 'ɪ˧˥': 55, '̩˧˩̰': 56, 'ɵ˧˥': 57, 'ei˧': 58, 'ɐu˧˩̰': 59, 'm̩˧': 60, 'ɐu˧˥': 61, 'ɐu˩˧': 62, 'ɐi˥': 63, 'ɔː˥': 64, 'ɔːĭ˧': 65, 'ou˧˥': 66, 'ou˩˧': 67, 'ɐ˧': 68, 'tsʰ': 69, 'ɛː˧˥': 70, 'iː˧˥': 71, 'ɔː˩˧': 72, 'kʰ': 73, 'ɐ˧˩̰': 74, 'aːŭ˧˥': 75, 'pʰ': 76, 'aːĭ˧˩̰': 77, 'ɵy˩˧': 78, 'ɵ˧': 79, 'ɛː˧': 80, 'ei˧˩̰': 81, 'uː˧˥': 82, 'ɔː˧˩̰': 83, 'ɛː˨': 84, 'uː˥': 85, 'ʊ˧': 86, 'iː˧˩̰': 87, 'yː˨': 88, 'aːŭ˧': 89, 'œː˩˧': 90, 'ɐ˧˥': 91, 'iː˩˧': 92, 'ɪ˧˩̰': 93, 'iːŭ˧˩̰': 94, 'œː˧˥': 95, 'yː˧': 96, 'uːĭ˩˧': 97, 'ɵy˧˥': 98, 'yː˧˩̰': 99, 'ɔːĭ˧˥': 100, 'ɛː': 101, 'u˨': 102, 'ou˧': 103, 'ei˨': 104, 'ɐu˥': 105, 'ɵ˥': 106, 'uː˧˩̰': 107, 'yː˥': 108, 'ɪ˥': 109, 'œː˥': 110, 'œː˧˩̰': 111, 'aːĭ˨': 112, 'ɐ˩˧': 113, 'œː˧': 114, 'uːĭ˧˥': 115, 'ɐu˧': 116, 'ɐi˧': 117, 'ou˧˩̰': 118, 'aːĭ˥': 119, 'aːŭ˥': 120, 'yː˧˥': 121, 'iːŭ˥': 122, 'ɔːĭ˨': 123, 'ʊ˧˥': 124, 'm̩˥': 125, 'iːŭ˧˥': 126, 'ɐi˩˧': 127, 'ɵy˥': 128, 'uːĭ˧': 129, 'ɵy˧˩̰': 130, 'uːĭ˥': 131, 'aːŭ˧˩̰': 132, 'yː˩˧': 133, 'ɔːĭ˧˩̰': 134, 'aːŭ˩˧': 135, 'aːĭ˩˧': 136, 'uːĭ˨': 137, 'œː˨': 138, 'uː˨': 139, 'ɵy˨': 140, 'aːŭ˨': 141, 'm̩˩˧': 142, 'ŋ˩˧': 143, 'ɪ˧': 144, 'm̩˨': 145, 'iːŭ˩˧': 146, 'iːŭ˨': 147, 'ɵ˨': 148, 'uːĭ˧˩̰': 149, 'uː˩˧': 150, 'ɵ˧˩̰': 151}\n",
      "Vocab size:  152\n",
      "Tokenizer for Cantonese pushed to the hub.\n",
      "\n",
      "Trainking tokenizer for Estonian...\n",
      "Tokens not found in phoible:  {'s^': 13520, 't^': 2185, 't^ː': 591, 'øi': 59, 'd^': 2004, 'æiː': 349}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'n': 4, 'o': 5, 'm': 6, 'i': 7, 's': 8, 'a': 9, 'æ': 10, 'r': 11, 'p': 12, 'l': 13, 'e': 14, 'd': 15, 'iː': 16, 't': 17, 'ʃ': 18, 'kː': 19, 'v': 20, 'æi': 21, 'pː': 22, 'u': 23, 'k': 24, 'h': 25, 'eː': 26, 'ɑ': 27, 'yi': 28, 'j': 29, 'uː': 30, 'ɛ': 31, 'ɡ': 32, 'b': 33, 'aː': 34, 'ɪ': 35, 'ɔ': 36, 'ɵ': 37, 'ɵː': 38, 'ʎ': 39, 'ʊ': 40, 'ø': 41, 'øː': 42, 'y': 43, 'yː': 44, 'tː': 45, 'ŋ': 46, 'oː': 47, 'ɲ': 48, 'w': 49, 'æː': 50, 'f': 51, 'ʒ': 52, 'z': 53}\n",
      "Vocab size:  54\n",
      "Tokenizer for Estonian pushed to the hub.\n",
      "\n",
      "Trainking tokenizer for Croatian...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'm': 4, 'i': 5, 'ʃ': 6, 'k': 7, 'o': 8, 'n': 9, 'ɛ': 10, 't': 11, 'd': 12, 'r': 13, 'æ': 14, 'ɪ': 15, 'p': 16, 'u': 17, 's': 18, 'v': 19, 'j': 20, 'e': 21, 't̠ʃ': 22, 'ɑ': 23, 'a': 24, 'ʊ': 25, 'ɾ': 26, 'l': 27, 'ɡ': 28, 'x': 29, 'ə': 30, 'tɕ': 31, 'ʒ': 32, 'b': 33, 'ts': 34, 'z': 35, 'dʑ': 36, 'f': 37, 'ɲ': 38, 'l̩': 39, 'h': 40, 'ʎ': 41, 'aː': 42, 'ŋ': 43, 'd̠ʒ': 44, 'oʊ': 45}\n",
      "Vocab size:  46\n",
      "Tokenizer for Croatian pushed to the hub.\n",
      "\n",
      "Trainking tokenizer for Danish...\n",
      "Tokens not found in phoible:  {'ɐ̯': 18929, 'ʔe': 19241, 'ʔu': 4971, '?ɑ': 6275, 'ε': 12789, 'ʔi': 6214, 'ʔʌ': 15181, 'ʔœ': 3426, 'ʔo': 3231, 'ʔy': 696, '?a': 447, 'ɒɒ': 325, 'ɑɑ': 150, 'aa': 13}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'd': 4, 'e': 5, 'ɛ': 6, 'n': 7, 'm': 8, 's': 9, 't': 10, 'k': 11, 'j': 12, 'f': 13, 'ɑ': 14, 'ɒ': 15, 'ə': 16, 'ʋ': 17, 'a': 18, 'l': 19, 'h': 20, 'b': 21, 'ʁ': 22, 'p': 23, 'œ': 24, 'i': 25, 'ɡ': 26, 'ʌ': 27, 'u': 28, 'ʃ': 29, 'ɔ': 30, 'w': 31, 'ð': 32, 'o': 33, 'y': 34, 'ŋ': 35, 'aɪ': 36, 'œː': 37, 'aː': 38, 'd̠ʒ': 39, 'uː': 40, 'ʌː': 41, 'ɜ': 42, 'oː': 43, 'yː': 44}\n",
      "Vocab size:  45\n",
      "Tokenizer for Danish pushed to the hub.\n",
      "\n",
      "Trainking tokenizer for Basque...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'oɪ': 4, 'a': 5, 'ɾ': 6, 'k': 7, 't̠ʃ': 8, 'i': 9, 's̺': 10, 'l': 11, 'p': 12, 'o': 13, 'r': 14, 'aɪ': 15, 'n': 16, 'm': 17, 'ð': 18, 'e': 19, 'ts̻': 20, 'β': 21, 's̻': 22, 'ʎ': 23, 'b': 24, 'aʊ': 25, 't': 26, 'ɣ': 27, 'ɡ': 28, 'c': 29, 'u': 30, 'eɪ': 31, 'd': 32, 'ts̺': 33, 'j': 34, 'ɲ': 35, 'f': 36, 'ʃ': 37, 'ɟ': 38, 'eʊ': 39, 'θ': 40, 'x': 41}\n",
      "Vocab size:  42\n",
      "Tokenizer for Basque pushed to the hub.\n",
      "\n",
      "Trainking tokenizer for Hungarian...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'm': 4, 'ɛ': 5, 'ɲ': 6, 'y': 7, 'n': 8, 'k': 9, 'ɑ': 10, 'r': 11, 'aː': 12, 'd': 13, 'i': 14, 'o': 15, 'h': 16, 'z': 17, 'v': 18, 'l': 19, 'eː': 20, 'j': 21, 'ʃ': 22, 'ɟ': 23, 's': 24, 'oː': 25, 'p': 26, 't': 27, 'tsː': 28, 'b': 29, 'u': 30, 'ɡ': 31, 'tː': 32, 'f': 33, 'ø': 34, 't̠ʃ': 35, 'uː': 36, 'iː': 37, 'ts': 38, 'ɟː': 39, 'yː': 40, 'øː': 41, 'ʎ': 42, 't̠ʃː': 43, 'c': 44, 'ɡː': 45, 'kː': 46, 'ɑː': 47, 'dː': 48, 'pː': 49, 'ʒ': 50, 'cː': 51, 'bː': 52}\n",
      "Vocab size:  53\n",
      "Tokenizer for Hungarian pushed to the hub.\n",
      "\n",
      "Trainking tokenizer for Turkish...\n",
      "Tokens not found in phoible:  {'ɯɯ': 30}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 's': 4, 'æ': 5, 'n': 6, 'o': 7, 'j': 8, 'ʊ': 9, 'ɔ': 10, 'a': 11, 'r': 12, 'm': 13, 'ɯ': 14, 'k': 15, 'ɪ': 16, 'l': 17, 'i': 18, 'ɛ': 19, 'v': 20, 'd': 21, 'd̠ʒ': 22, 'y': 23, 't': 24, 'b': 25, 'u': 26, 'z': 27, 'ʃ': 28, 'ɟ': 29, 'e': 30, 'p': 31, 'ɡ': 32, 'ɫ': 33, 'h': 34, 't̠ʃ': 35, 'ɾ': 36, 'f': 37, 'ø': 38, 'œ': 39, 'aː': 40, 'c': 41, 'ʊː': 42, 'tː': 43, 'oː': 44, 'œː': 45, 'ʒ': 46}\n",
      "Vocab size:  47\n",
      "Tokenizer for Turkish pushed to the hub.\n",
      "\n",
      "Trainking tokenizer for Farsi...\n",
      "Tokens not found in phoible:  {'1': 242}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'k': 4, 's': 5, 'o': 6, 'b': 7, 'a': 8, 'h': 9, 'n': 10, 't̠ʃ': 11, 'i': 12, 'j': 13, 'd': 14, 'e': 15, 'ʃ': 16, 'u': 17, 'ɡ': 18, 'r': 19, 'f': 20, 't': 21, 'm': 22, 'd̠ʒ': 23, 'l': 24, 'q': 25, 'v': 26, 'z': 27, 'p': 28}\n",
      "Vocab size:  29\n",
      "Tokenizer for Farsi pushed to the hub.\n",
      "\n",
      "Trainking tokenizer for Icelandic...\n",
      "Tokens not found in phoible:  {'r#': 3506, 'eɪː': 7584, 'n#': 5999, 'ŋ#': 169, 'aʊː': 13917, 'oʊː': 2679, 'aɪː': 1503, 'øyː': 662, 'l#': 2290, 'tl#': 1052, 'm#': 146}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'aː': 4, 'ɾ': 5, 'r': 6, 'ɪ': 7, 'ɛ': 8, 'd': 9, 's': 10, 'j': 11, 'a': 12, 'b': 13, 'iː': 14, 'k': 15, 'ʋ': 16, 'ɛː': 17, 'θ': 18, 'i': 19, 'l': 20, 'n': 21, 'uː': 22, 'ð': 23, 'ɡ': 24, 'ɔ': 25, 'h': 26, 'aʊ': 27, 'y': 28, 'm': 29, 'f': 30, 'ɔː': 31, 'x': 32, 'ɟ': 33, 't': 34, 'eɪ': 35, 'oʊ': 36, 'p': 37, 'ŋ': 38, 'ɣ': 39, 'yː': 40, 'u': 41, 'ɪː': 42, 'œ': 43, 'aɪ': 44, 'ç': 45, 'ə': 46, 'øy': 47, 'c': 48, 'ɲ': 49, 'œː': 50, 'ɔɪ': 51}\n",
      "Vocab size:  52\n",
      "Tokenizer for Icelandic pushed to the hub.\n",
      "\n",
      "Trainking tokenizer for Indonesian...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'n': 4, 'i': 5, 'h': 6, 'l': 7, 'o': 8, 't': 9, 'm': 10, 'a': 11, 'w': 12, 's': 13, 'd̠ʒ': 14, 'ŋ': 15, 'd': 16, 'p': 17, 'ɛ': 18, 'ɡ': 19, 'b': 20, 'u': 21, 'r': 22, 'au̯': 23, 'z': 24, 'k': 25, 'ɲ': 26, 'j': 27, 't̠ʃ': 28, 'e': 29, 'ə': 30, 'ɔ': 31, 'ai̯': 32, 'f': 33, 'v': 34, 'ʔ': 35, 'ʃ': 36, 'χ': 37, 'x': 38}\n",
      "Vocab size:  39\n",
      "Tokenizer for Indonesian pushed to the hub.\n",
      "\n",
      "Trainking tokenizer for Irish...\n",
      "Tokens not found in phoible:  {'A': 4003}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'ɡ': 4, 'ə': 5, 's': 6, 'n': 7, 'w': 8, 'ɪ': 9, 'l': 10, 'oː': 11, 'ɛ': 12, 'ɑː': 13, 'i̯': 14, 'ʒ': 15, 'b': 16, 'a': 17, 'm': 18, 'ʃ': 19, 'c': 20, 'x': 21, 't̪': 22, 'ʊ': 23, 'iː': 24, 'ɹ': 25, 'eː': 26, 'h': 27, 'ɐ': 28, 'r': 29, 'f': 30, 'ɔ': 31, 'd̪': 32, 'k': 33, 'lʲ': 34, 'ɡʲ': 35, 'hʲ': 36, 'kʲ': 37, 'dʲ': 38, 'v': 39, 'nʲ': 40, 'p': 41, 'd̠ʒ': 42, 'uː': 43, 'ŋ': 44, 'ɣ': 45, 'd': 46, 'ʁ': 47, 'j': 48, 'eɪ': 49, 't̠ʃ': 50, 'aʊ': 51, 'iə': 52, 'çʲ': 53, 'pʲ': 54, 'χ': 55, 'uə': 56, 'aɪ': 57, 't': 58, 'u': 59, 'ŭ': 60, 'bʲ': 61, 'fʲ': 62, 'z': 63}\n",
      "Vocab size:  64\n",
      "Tokenizer for Irish pushed to the hub.\n",
      "\n",
      "Trainking tokenizer for Welsh...\n",
      "Tokens not found in phoible:  {'ɔɨ': 2852, 'ɑɨ': 5885, 'ɨu': 2984}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'd': 4, 'eː': 5, 'ç': 6, 'r': 7, 'aɨ': 8, 'ɛ': 9, 't': 10, 'ɔ': 11, 'w': 12, 'a': 13, 'n': 14, 'j': 15, 'aʊ': 16, 'ə': 17, 'ð': 18, 'oː': 19, 'iː': 20, 'ɨ': 21, 'b': 22, 's': 23, 'ø': 24, 'ɡ': 25, 'uɨ': 26, 'h': 27, 'ʊ': 28, 'm': 29, 'əɪ': 30, 'θ': 31, 'l': 32, 'v': 33, 'ɨː': 34, 'ʌ': 35, 'k': 36, 'ɬ': 37, 'p': 38, 'f': 39, 'x': 40, 'uː': 41, 'ʃ': 42, 'aɪ': 43, 'əɨ': 44, 'ɑː': 45, 'ɔɪ': 46, 'eʊ': 47, 'ŋ': 48, 'd̠ʒ': 49, 'ɪu': 50, 'z': 51}\n",
      "Vocab size:  52\n",
      "Tokenizer for Welsh pushed to the hub.\n",
      "\n",
      "Trainking tokenizer for Korean...\n",
      "Tokens not found in phoible:  {'ph': 5046, 'kh': 8644}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'w': 4, 'ɐ': 5, 'j': 6, 'o': 7, 'ɾ': 8, 'i': 9, 'h': 10, 'n': 11, 'ɯ': 12, 'd': 13, 'e': 14, 'ŋ': 15, 'p': 16, 'dʑ': 17, 'u': 18, 'm': 19, 'q': 20, 'ʌ': 21, 'ɫ': 22, 't̠ʃ': 23, 'tɕ': 24, 'ɛ': 25, 'b': 26, 's': 27, 'k': 28, 't': 29, 'l': 30, 'ɡ': 31, 'hʲ': 32}\n",
      "Vocab size:  33\n",
      "Tokenizer for Korean pushed to the hub.\n",
      "\n",
      "Trainking tokenizer for Swedish...\n",
      "Tokens not found in phoible:  {'sx': 2072}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'ɔ': 4, 'j': 5, 'k': 6, 'l': 7, 'ɛ': 8, 'm': 9, 'd': 10, 'ə': 11, 'ʉ': 12, 'f': 13, 'ɪ': 14, 'ŋ': 15, 'r': 16, 'a': 17, 'n': 18, 'iː': 19, 'ɑː': 20, 't': 21, 'ɛː': 22, 's': 23, 'v': 24, 'oː': 25, 'uː': 26, 'eː': 27, 'ʊ': 28, 'p': 29, 'b': 30, 'h': 31, 'øː': 32, 'yː': 33, 'ɡ': 34, 'ɵ': 35, 'ʃ': 36, 'ʂ': 37, 'œ': 38, 'tː': 39, 'kː': 40, 'y': 41, 'ɕ': 42, 'ɵː': 43}\n",
      "Vocab size:  44\n",
      "Tokenizer for Swedish pushed to the hub.\n",
      "\n",
      "Trainking tokenizer for Norwegian...\n",
      "Tokens not found in phoible:  {'ʉɪ': 11}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 't': 4, 'ɑ': 5, 'kː': 6, 'v': 7, 'a': 8, 'r': 9, 'ʃ': 10, 'oː': 11, 'ɡ': 12, 'uː': 13, 'd': 14, 'h': 15, 'ʉː': 16, 's': 17, 'ɛ': 18, 'tː': 19, 'eː': 20, 'n': 21, 'pː': 22, 'ə': 23, 'l': 24, 'ɪ': 25, 'b': 26, 'iː': 27, 'ɛː': 28, 'j': 29, 'k': 30, 'ʉ': 31, 'ɔ': 32, 'm': 33, 'ʊː': 34, 'f': 35, 'yː': 36, 'aɪ': 37, 'p': 38, 'øy': 39, 'ŋ': 40, 'øː': 41, 'dː': 42, 'œ': 43, 'bː': 44, 'x': 45, 'aː': 46, 'ɑː': 47, 'y': 48, 'aʊ': 49, 'ʊ': 50, 'ɡː': 51, 'ɔɪ': 52, 'ɹ': 53, 'ʂ': 54, 'ɑɪ': 55, 'w': 56}\n",
      "Vocab size:  57\n",
      "Tokenizer for Norwegian pushed to the hub.\n",
      "\n",
      "Trainking tokenizer for Quechua...\n",
      "Tokens not found in phoible:  {'t`': 56, 'k`': 75, 'p`': 54, 't̠ʃ`': 42, 'q`': 76}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 't': 4, 's': 5, 'aː': 6, 'h': 7, 'u': 8, 'ʎ': 9, 'j': 10, 'a': 11, 'm': 12, 'k': 13, 'i': 14, 'n': 15, 'r': 16, 'p': 17, 'ʔ': 18, 'q': 19, 't̠ʃ': 20, 'f': 21, 'e': 22, 'b': 23, 'w': 24, 'ɡ': 25, 'l': 26, 'v': 27, 'd': 28, 'iː': 29, 'o': 30, 'z': 31}\n",
      "Vocab size:  32\n",
      "Tokenizer for Quechua pushed to the hub.\n",
      "\n",
      "Trainking tokenizer for Catalan...\n",
      "Tokens not found in phoible:  {}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 't': 4, 'ɛ': 5, 'ð': 6, 'o': 7, 'n': 8, 'ɐ': 9, 'ɡ': 10, 'ɾ': 11, 'a': 12, 's': 13, 'j': 14, 'ə': 15, 'ʋ': 16, 'i': 17, 'ŋ': 18, 'p': 19, 'u': 20, 'ʑ': 21, 'k': 22, 'z': 23, 'w': 24, 'm': 25, 'ɣ': 26, 'ʎ': 27, 'l': 28, 'β': 29, 'ɕ': 30, 'f': 31, 'r': 32, 'd': 33, 'dʑ': 34, 'ʊ': 35, 'e': 36, 'ɲ': 37, 'ts': 38, 'b': 39, 'tɕ': 40, 'ɔ': 41, 'pː': 42}\n",
      "Vocab size:  43\n",
      "Tokenizer for Catalan pushed to the hub.\n",
      "\n",
      "Trainking tokenizer for Italian...\n",
      "Tokens not found in phoible:  {'ss': 2547}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'i': 4, 'l': 5, 't': 6, 'a': 7, 'o': 8, 'k': 9, 'ɔ': 10, 'z': 11, 'f': 12, 'v': 13, 'n': 14, 'e': 15, 'd': 16, 'j': 17, 't̠ʃ': 18, 'ɪ': 19, 'b': 20, 'ɛ': 21, 'm': 22, 'w': 23, 's': 24, 'ɛː': 25, 'p': 26, 'r': 27, 'u': 28, 'kː': 29, 'ʊ': 30, 'ʎ': 31, 'd̠ʒ': 32, 'tː': 33, 'pː': 34, 'ts': 35, 'aː': 36, 'oː': 37, 'iː': 38, 't̠ʃː': 39, 'ɾ': 40, 'ɡ': 41, 'eː': 42, 'dz': 43, 'bː': 44, 'd̠ʒː': 45, 'ɲ': 46, 'tsː': 47, 'ʃ': 48, 'eɪ': 49, 'd̪': 50, 'dzː': 51, 'ŋ': 52, 'dː': 53, 'aɪ': 54, 'uː': 55, 'ɔː': 56, 'ɡː': 57}\n",
      "Vocab size:  58\n",
      "Tokenizer for Italian pushed to the hub.\n",
      "\n",
      "Trainking tokenizer for PortuguesePt...\n",
      "Tokens not found in phoible:  {'ɐ̃ʊ̃': 19619}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'f': 4, 'ɨ': 5, 'ʃ': 6, 'ɐ': 7, 'a': 8, 's': 9, 'i': 10, 'ŋ': 11, 'n': 12, 'd': 13, 'p': 14, 'oɪ': 15, 'z': 16, 'ɛ': 17, 't': 18, 'ɐ̃': 19, 'm': 20, 'b': 21, 'eɪ': 22, 'ʒ': 23, 'l': 24, 'ɡ': 25, 'w': 26, 'ɔ': 27, 'ɾ': 28, 'e': 29, 'aɪ': 30, 'eʊ': 31, 'ʊ': 32, 'ɛʊ': 33, 'ũ': 34, 'u': 35, 'ɹ': 36, 'o': 37, 'ʎ': 38, 'ə': 39, 'k': 40, 'ɲ': 41, 'aː': 42, 'v': 43, 'aʊ': 44, 'ʁ': 45, 'j': 46, 'iʊ': 47, 'ɑ': 48, 'õ': 49, 'uɪ': 50, 'mʲ': 51, 'ɛɪ': 52, 'r': 53, 'ɔɪ': 54, 't̠ʃ': 55, 'ts': 56, 'rʲ': 57, 'sʲ': 58}\n",
      "Vocab size:  59\n",
      "Tokenizer for PortuguesePt pushed to the hub.\n",
      "\n",
      "Trainking tokenizer for PortugueseBr...\n",
      "Tokens not found in phoible:  {'ɐ̃ʊ̃': 890}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'k': 4, 'y': 5, 'm': 6, 'aɪ': 7, 's': 8, 't': 9, 'eɪ': 10, 'ŋ': 11, 'a': 12, 'i': 13, 'n': 14, 'ɛ': 15, 'æ': 16, 'z': 17, 'ɡ': 18, 'r': 19, 'v': 20, 'u': 21, 'ɾ': 22, 'd': 23, 'ʊ': 24, 'oɪ': 25, 'ɲ': 26, 'e': 27, 'f': 28, 'o': 29, 'p': 30, 'ʒ': 31, 't̠ʃ': 32, 'd̠ʒ': 33, 'eʊ': 34, 'w': 35, 'aʊ': 36, 'j': 37, 'ə': 38, 'ũ': 39, 'ɐ̃': 40, 'l': 41, 'b': 42, 'x': 43, 'ɔ': 44, 'ʃ': 45, 'iʊ': 46, 'ɛʊ': 47, 'ɔɪ': 48, 'uɪ': 49}\n",
      "Vocab size:  50\n",
      "Tokenizer for PortugueseBr pushed to the hub.\n",
      "\n",
      "Trainking tokenizer for Romanian...\n",
      "Tokens not found in phoible:  {'tsʲʲ': 705, 'zʲʲ': 623, 'ʃʲʲ': 91, 'nʲʲ': 291, 'tʲʲ': 273, 'mʲʲ': 108, 'ɾʲʲ': 178, 'bʲʲ': 86, 'dʲʲ': 33, 'pʲʲ': 80, 'lʲʲ': 68, 'fʲʲ': 20}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'a': 4, 'ʃ': 5, 'm': 6, 'e': 7, 'h': 8, 'd': 9, 's': 10, 'f': 11, 't̠ʃ': 12, 'l': 13, 'ə': 14, 'p': 15, 'i': 16, 'k': 17, 'ɨ': 18, 'n': 19, 'uɪ': 20, 't': 21, 't̠ʃʲ': 22, 'w': 23, 'u': 24, 'ts': 25, 'aɪ': 26, 'r': 27, 'o': 28, 'j': 29, 'b': 30, 'v': 31, 'eɪ': 32, 'ɔa': 33, 'ɡ': 34, 'z': 35, 'ɾ': 36, 'ea': 37, 'iɪ': 38, 'aʊ': 39, 'ʒ': 40, 'əɪ': 41, 'd̠ʒ': 42, 'eʊ': 43, 'iʊ': 44, 'tʲ': 45, 'ŋ': 46, 'tsʲ': 47, 'kʲ': 48, 'eo': 49, 'd̠ʒʲ': 50, 'rʲ': 51, 'əʊ': 52, 'oʊ': 53, 'sʲ': 54, 'ɾʲ': 55, 'ɔ': 56, 'nʲ': 57, 'oɪ': 58}\n",
      "Vocab size:  59\n",
      "Tokenizer for Romanian pushed to the hub.\n",
      "\n",
      "Trainking tokenizer for all languages...\n",
      "Tokens not found in phoible:  {'ʌʊ': 29128, 'yʊ': 655, 'a˥˩': 134103, 'ɻ̩˥˩': 60603, 'ə˧˥': 47575, 'e˧˥': 8803, 'ɤ˥˩': 94372, 'o˥': 17623, 'a˥': 136085, 'ə˥': 30513, 'ʊ˥': 15404, 'au̯˧˩˧': 73681, 'u˧˩˧': 8131, 'i˥': 57614, 'ei̯˧˩˧': 13610, 'i˧˥': 23314, 'ai̯˧˥': 43322, 'ou̯˧˩˧': 42463, 'u˧˥': 18595, 'ɤ˧˥': 5204, 'o˧˩˧': 41410, 'au̯˥˩': 60495, 'ə˧˩˧': 18898, 'ɤ˥': 8992, 'ei̯˧˥': 19803, 'ʊ˧˥': 8683, 'i˧˩˧': 105629, 'ɻ̩˧˩˧': 3656, 'ei̯˥˩': 38649, 'u˥˩': 41026, 'ai̯˥': 13886, 'u˥': 12807, 'a˧˩˧': 46844, 'a˧˥': 44695, 'ai̯˥˩': 49896, 'ɛ˥˩': 19495, 'i˥˩': 19548, 'y˥˩': 17253, 'ou̯˥˩': 35280, 'e˥': 4690, 'ɹ̩˥˩': 6722, 'ɛ˧˥': 8767, 'au̯˥': 15809, 'ou̯˧˥': 9856, 'e˧˩˧': 9647, 'ɛ˥': 15832, 'ɻ̩˥': 22908, 'ɹ̩˧˩˧': 2357, 'au̯˧˥': 3898, 'ai̯˧˩˧': 4540, 'ou̯˥': 10351, 'o˥˩': 25467, 'ɛ˧˩˧': 8759, 'ʊ˧˩˧': 4164, 'ɔ˥': 10473, 'ə˥˩': 4826, 'ʊ˥˩': 9913, 'ɤ˧˩˧': 9014, 'o˧˥': 3316, 'ei̯˥': 7763, 'e˥˩': 4552, 'ɚ˧˩˧': 309, 'y˥': 3321, 'ɚ˥˩': 1477, 'y˧˥': 2761, 'y˧˩˧': 1124, 'ɹ̩˥': 1020, 'ɻ̩˧˥': 8558, 'ɚ˧˥': 1378, 'ɔ˥˩': 72, 'ɹ̩˧˥': 52, 'm˧˥': 12, 'g': 58660, 't͡s': 22642, 'gʲ': 1102, 'aː˧': 59038, 'ɐ˥': 21803, 'aː˧˩̰': 15197, 'ɛː˥': 27225, 'ei˩˧': 22657, 'aː˨': 5304, 'ɐi˧˥': 13853, 'm̩˧˥': 1244, 'ou˥': 3421, 'aː˧˥': 8982, 'ei˥': 5487, 'iː˧': 4689, 'ɔː˧˥': 12441, 'iː˥': 24769, 'aːĭ˧': 4468, 'ɐ˨': 7396, 'ɵy˧': 5065, 'aː˥': 32496, 'ou˨': 11435, 'ɔː˧': 24130, 'ɐi˧˩̰': 5346, 'uː˧': 2278, 'ɔːĭ˥': 2298, 'ɐu˨': 4149, 'iː˨': 5126, 'ei˧˥': 8635, 'ɐi˨': 13031, 'ʊ˧˩̰': 2859, 'ʊ˨': 4273, 'aː˩˧': 5389, 'aːĭ˧˥': 3325, 'ɔː˨': 3349, 'ɛː˩˧': 7315, 'ɪ˨': 6431, 'iːŭ˧': 4545, 'ɛː˧˩̰': 2884, 'ɪ˧˥': 1925, '̩˧˩̰': 22726, 'ɵ˧˥': 267, 'ei˧': 1143, 'ɐu˧˩̰': 2432, 'm̩˧': 1175, 'ɐu˧˥': 3227, 'ɐu˩˧': 8401, 'ɐi˥': 8675, 'ɔː˥': 11128, 'ɔːĭ˧': 1102, 'ou˧˥': 22062, 'ou˩˧': 6647, 'ɐ˧': 8489, 'ɛː˧˥': 4262, 'iː˧˥': 6385, 'ɔː˩˧': 11133, 'ɐ˧˩̰': 2664, 'aːŭ˧˥': 849, 'aːĭ˧˩̰': 2281, 'ɵy˩˧': 8042, 'ɵ˧': 2373, 'ɛː˧': 4946, 'ei˧˩̰': 4084, 'uː˧˥': 1137, 'ɔː˧˩̰': 3744, 'ɛː˨': 1157, 'uː˥': 971, 'ʊ˧': 1148, 'iː˧˩̰': 4114, 'yː˨': 3341, 'aːŭ˧': 683, 'œː˩˧': 1290, 'ɐ˧˥': 6105, 'iː˩˧': 1543, 'ɪ˧˩̰': 1450, 'iːŭ˧˩̰': 780, 'œː˧˥': 3282, 'yː˧': 1110, 'uːĭ˩˧': 894, 'ɵy˧˥': 1296, 'yː˧˩̰': 3605, 'ɔːĭ˧˥': 279, 'u˨': 151, 'ou˧': 1533, 'ei˨': 3433, 'ɐu˥': 887, 'ɵ˥': 2077, 'uː˧˩̰': 506, 'yː˥': 1527, 'ɪ˥': 4948, 'œː˥': 1257, 'œː˧˩̰': 1552, 'aːĭ˨': 1589, 'ɐ˩˧': 63, 'œː˧': 2729, 'uːĭ˧˥': 21, 'ɐu˧': 1022, 'ɐi˧': 1785, 'ou˧˩̰': 475, 'aːĭ˥': 1205, 'aːŭ˥': 1044, 'yː˧˥': 1060, 'iːŭ˥': 548, 'ɔːĭ˨': 417, 'm̩˥': 376, 'iːŭ˧˥': 1108, 'ɐi˩˧': 505, 'ɵy˥': 412, 'uːĭ˧': 33, 'ɵy˧˩̰': 739, 'uːĭ˥': 658, 'aːŭ˧˩̰': 30, 'yː˩˧': 261, 'ɔːĭ˧˩̰': 223, 'aːŭ˩˧': 272, 'aːĭ˩˧': 1112, 'uːĭ˨': 114, 'œː˨': 952, 'uː˨': 474, 'ɵy˨': 321, 'aːŭ˨': 81, 'm̩˩˧': 93, 'ŋ˩˧': 286, 'ɪ˧': 266, 'm̩˨': 261, 'iːŭ˩˧': 33, 'iːŭ˨': 139, 'ɵ˨': 16, 'uːĭ˧˩̰': 47, 'uː˩˧': 15, 'ɵ˧˩̰': 91, 's^': 13520, 't^': 2185, 't^ː': 591, 'øi': 59, 'd^': 2004, 'æiː': 349, 'ɐ̯': 18929, 'ʔe': 19241, 'ʔu': 4971, '?ɑ': 6275, 'ε': 12789, 'ʔi': 6214, 'ʔʌ': 15181, 'ʔœ': 3426, 'ʔo': 3231, 'ʔy': 696, '?a': 447, 'ɒɒ': 325, 'ɑɑ': 150, 'aa': 13, 'ɯɯ': 30, '1': 242, 'r#': 3506, 'eɪː': 7584, 'n#': 5999, 'ŋ#': 169, 'aʊː': 13917, 'oʊː': 2679, 'aɪː': 1503, 'øyː': 662, 'l#': 2290, 'tl#': 1052, 'm#': 146, 'A': 4003, 'lʲʲ': 71, 'ɔɨ': 2852, 'ɑɨ': 5885, 'ɨu': 2984, 'ph': 5046, 'kh': 8644, 'sx': 2072, 'ʉɪ': 11, 't`': 56, 'k`': 75, 'p`': 54, 't̠ʃ`': 42, 'q`': 76, 'ss': 2547, 'ɐ̃ʊ̃': 20509, 'tsʲʲ': 705, 'zʲʲ': 623, 'ʃʲʲ': 91, 'nʲʲ': 291, 'tʲʲ': 273, 'mʲʲ': 108, 'ɾʲʲ': 178, 'bʲʲ': 86, 'dʲʲ': 33, 'pʲʲ': 80, 'fʲʲ': 20}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 's': 4, 'iː': 5, 'ð': 6, 'ɛ': 7, 'ɹ': 8, 'z': 9, 'ʌ': 10, 'f': 11, 'eɪ': 12, 'w': 13, 'ɪ': 14, 'ɡ': 15, 'l': 16, 'æ': 17, 'ɑ': 18, 'h': 19, 'ə': 20, 'ʊ': 21, 'k': 22, 'p': 23, 'uː': 24, 'b': 25, 'i': 26, 't': 27, 'aɪ': 28, 'θ': 29, 'ŋ': 30, 'j': 31, 'ɔ': 32, 'm': 33, 'ɔɪ': 34, 'n': 35, 'd': 36, 'oʊ': 37, 'aʊ': 38, 'v': 39, 'ɜː': 40, 't̠ʃ': 41, 'd̠ʒ': 42, 'ʃ': 43, 'iə': 44, 'ʒ': 45, 'ɑ̃': 46, 'r': 47, 'x': 48, 'nʲ': 49, 'ɒ': 50, 'a': 51, 'ɑː': 52, 'ɔː': 53, 'əʊ': 54, 'ɐ': 55, 'eə': 56, 'ʊə': 57, 'n̩': 58, 'aː': 59, 'y': 60, 'ɛ̃': 61, 'ʁ': 62, 'e': 63, 'ɔ̃': 64, 'u': 65, 'o': 66, 'œ̃': 67, 'ø': 68, 'œ': 69, 'oː': 70, 'yː': 71, 'ɲ': 72, 'ts': 73, 'eː': 74, 'ʀ': 75, 'ç': 76, 'ɛɪ': 77, 'ʏ': 78, 'ɛː': 79, 'pf': 80, 'øː': 81, 'ã': 82, 'ɾ': 83, 'β': 84, 'ʎ': 85, 'ɣ': 86, 'ʝ': 87, 'oɪ': 88, 'eʊ': 89, 'pː': 90, 'ɟ': 91, 'ʋ': 92, 'ɪː': 93, 'ɵ': 94, 'œy': 95, 'tʲ': 96, 'au̯': 97, 'ʂ': 98, 'ɤ': 99, 'kʰ': 100, 'ʈʂʰ': 101, 'ɕ': 102, 'pʰ': 103, 'tɕ': 104, 'ʈʂ': 105, 'ɹ̩': 106, 'tɕʰ': 107, 'tʰ': 108, 'ɻ': 109, 'ɥ': 110, 'tsʰ': 111, 'ei̯': 112, 'ou̯': 113, 'ɻ̩': 114, 'ai̯': 115, 'ɯː': 116, 'ɯ': 117, 'pʲ': 118, 'ɸ': 119, 'rʲ': 120, 'kʲ': 121, 'bʲ': 122, 'mʲ': 123, 'kː': 124, 'æi': 125, 'yi': 126, 'ɵː': 127, 'tː': 128, 'æː': 129, 'dʑ': 130, 'l̩': 131, 'œː': 132, 'ʌː': 133, 'ɜ': 134, 'ʔ': 135, 's̺': 136, 'ts̻': 137, 's̻': 138, 'c': 139, 'ts̺': 140, 'tsː': 141, 'ɟː': 142, 't̠ʃː': 143, 'ɡː': 144, 'dː': 145, 'cː': 146, 'bː': 147, 'dzː': 148, 'ɫ': 149, 'ʊː': 150, 'q': 151, 'øy': 152, 'χ': 153, 'i̯': 154, 't̪': 155, 'd̪': 156, 'lʲ': 157, 'ɡʲ': 158, 'hʲ': 159, 'dʲ': 160, 'çʲ': 161, 'uə': 162, 'ŭ': 163, 'fʲ': 164, 'aɨ': 165, 'ɨ': 166, 'uɨ': 167, 'əɪ': 168, 'ɨː': 169, 'ɬ': 170, 'əɨ': 171, 'ɪu': 172, 'ʉ': 173, 'ʉː': 174, 'ɑɪ': 175, 'ʑ': 176, 'dz': 177, 'd̠ʒː': 178, 'ɐ̃': 179, 'ɛʊ': 180, 'ũ': 181, 'iʊ': 182, 'õ': 183, 'uɪ': 184, 'sʲ': 185, 't̠ʃʲ': 186, 'ɔa': 187, 'ea': 188, 'iɪ': 189, 'tsʲ': 190, 'eo': 191, 'd̠ʒʲ': 192, 'ɾʲ': 193}\n",
      "Vocab size:  194\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "for language, dataset in zip(languages, datasets):\n",
    "    print(f'\\nTrainking tokenizer for {language}...')\n",
    "    allow_non_phoible = language in ['Mandarin', 'Cantonese'] # For Mandarin and Cantonese, allow non-phoible tokens since we merge tone with vowels\n",
    "    vocab = build_vocabulary([dataset], allow_non_phoible=allow_non_phoible)\n",
    "    tokenizer = build_phoneme_tokenizer(vocab)\n",
    "    tokenizer.push_to_hub(f\"transformersegmentation/CHILDES-{language}-phoneme-tokenizer\")\n",
    "    print(f'Tokenizer for {language} pushed to the hub.')\n",
    "\n",
    "print(f'\\nTrainking tokenizer for all languages...')\n",
    "vocab = build_vocabulary(datasets)\n",
    "tokenizer = build_phoneme_tokenizer(vocab)\n",
    "tokenizer.push_to_hub(\"transformersegmentation/CHILDES-phoneme-tokenizer\")\n",
    "print('Done.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BPE Tokenizers for CHILDES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading readme: 100%|██████████| 11.1k/11.1k [00:00<00:00, 6.42MB/s]\n",
      "Downloading data: 100%|██████████| 597M/597M [00:28<00:00, 20.9MB/s] \n",
      "Downloading data: 100%|██████████| 3.62M/3.62M [00:00<00:00, 7.24MB/s]\n",
      "Generating train split: 1635797 examples [00:09, 179180.63 examples/s]\n",
      "Generating valid split: 10000 examples [00:00, 158621.60 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset('transformersegmentation/CHILDES', 'English', split='train')\n",
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "        [normalizers.NFD(),\n",
    "         normalizers.Lowercase(),\n",
    "         normalizers.Strip(),\n",
    "         normalizers.StripAccents(),\n",
    "        ]\n",
    "    )\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
    "\n",
    "trainer = trainers.BpeTrainer(vocab_size=8192, special_tokens=[\"UTT_BOUNDARY\", \"PAD\", \"UNK\"])\n",
    "tokenizer.train_from_iterator(dataset['processed_gloss'], trainer=trainer)\n",
    "\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"UTT_BOUNDARY $A\",\n",
    "    pair=\"UTT_BOUNDARY $A UTT_BOUNDARY $B:1\",\n",
    "    special_tokens=[(\"UTT_BOUNDARY\", tokenizer.token_to_id(\"UTT_BOUNDARY\"))],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: we're gonna get some clothes for thomas.\n",
      "['UTT_BOUNDARY', 'Ġwe', \"'re\", 'Ġgonna', 'Ġget', 'Ġsome', 'Ġclothes', 'Ġfor', 'Ġthomas', '.']\n"
     ]
    }
   ],
   "source": [
    "example = dataset['processed_gloss'][300]\n",
    "encoding = tokenizer.encode(example)\n",
    "print(f'Example: {example}')\n",
    "print(encoding.tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/transformersegmentation/CHILDES-English-BPE-gloss-tokenizer/commit/90f971556e7648c3c03e704268f1a281c47ef676', commit_message='Upload tokenizer', commit_description='', oid='90f971556e7648c3c03e704268f1a281c47ef676', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer, pad_token='PAD', unk_token='UNK', bos_token='UTT_BOUNDARY', eos_token='UTT_BOUNDARY', add_prefix_space=True)\n",
    "wrapped_tokenizer.push_to_hub(\"transformersegmentation/CHILDES-English-BPE-gloss-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 90, 152, 188, 166, 197, 1217, 190, 1863, 6, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized = wrapped_tokenizer(example, padding='max_length', max_length=20, truncation=True, add_special_tokens=True)\n",
    "tokenized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UTT_BOUNDARY',\n",
       " 'Ġwe',\n",
       " \"'re\",\n",
       " 'Ġgonna',\n",
       " 'Ġget',\n",
       " 'Ġsome',\n",
       " 'Ġclothes',\n",
       " 'Ġfor',\n",
       " 'Ġthomas',\n",
       " '.',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD',\n",
       " 'PAD']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer.convert_ids_to_tokens(tokenized['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [0, 115, 91, 45, 3501, 37, 6], 'attention_mask': [1, 1, 1, 1, 1, 1, 1]}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer('this is a test .')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
