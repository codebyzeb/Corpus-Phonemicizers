{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Tokenizers for the BabyLM dataset\n",
    "\n",
    "We create eight tokenizers:\n",
    "* A BPE tokenizer for orthographic text (keeps spaces)\n",
    "* A BPE tokenizer for orthographic text (removes spaces)\n",
    "* A character-based tokenizer for orthographic text (keeps spaces)\n",
    "* A character-based tokenizer for orthographic text (removes spaces)\n",
    "* A BPE tokenizer for phonemes (keeps spaces)\n",
    "* A BPE tokenizer for phonemes (removes spaces)\n",
    "* A character-based tokenizer for phonemes (keeps spaces)\n",
    "* A character-based tokenizer for phonemes (removes spaces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "from datasets import load_dataset\n",
    "from tokenizers import Tokenizer, models, normalizers, pre_tokenizers, trainers, processors, decoders\n",
    "from transformers import GPT2TokenizerFast\n",
    "\n",
    "dataset = load_dataset('BabyLM-phonemized', 'strict', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_example(example, tokenizer):\n",
    "    tokenized = tokenizer(example)[\"input_ids\"]\n",
    "    print(f\"Original: {example}\")\n",
    "    print(f\"Ids: {tokenized}\")\n",
    "    print(f\"Tokens: {tokenizer.convert_ids_to_tokens(tokenized)}\")\n",
    "    print(f\"Decoded: {tokenizer.decode(tokenized)}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPE tokenizer for orthographic text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "        [normalizers.NFD(),\n",
    "         normalizers.Lowercase(),\n",
    "         normalizers.Strip(),\n",
    "         normalizers.StripAccents(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
    "trainer = trainers.BpeTrainer(vocab_size=8192, special_tokens=[\"UTT_BOUNDARY\", \"PAD\", \"UNK\"])\n",
    "tokenizer.train_from_iterator(dataset['text'], trainer=trainer)\n",
    "\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"UTT_BOUNDARY $A\",\n",
    "    pair=\"UTT_BOUNDARY $A UTT_BOUNDARY $B:1\",\n",
    "    special_tokens=[(\"UTT_BOUNDARY\", tokenizer.token_to_id(\"UTT_BOUNDARY\"))],\n",
    ")\n",
    "\n",
    "wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer, pad_token='PAD', unk_token='UNK', bos_token='UTT_BOUNDARY', eos_token='UTT_BOUNDARY', add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: who's that?\n",
      "Ids: [0, 398, 237, 238, 33]\n",
      "Tokens: ['UTT_BOUNDARY', 'Ġwho', \"'s\", 'Ġthat', '?']\n",
      "Decoded: UTT_BOUNDARY who's that?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_example(dataset['text'][6], wrapped_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/transformersegmentation/BabyLM-BPE-ortho-tokenizer/commit/ae0bd4d3959981c5a6b621ac2539d0271f311ef9', commit_message='Upload tokenizer', commit_description='', oid='ae0bd4d3959981c5a6b621ac2539d0271f311ef9', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer.push_to_hub(\"transformersegmentation/BabyLM-BPE-ortho-tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPE tokenizer for orthographic text without spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "        [normalizers.Replace(\" \", \"\"),\n",
    "         normalizers.Replace(\"\\t\", \"\"),\n",
    "         normalizers.NFD(),\n",
    "         normalizers.Lowercase(),\n",
    "         normalizers.Strip(),\n",
    "         normalizers.StripAccents(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False, use_regex=True)\n",
    "trainer = trainers.BpeTrainer(vocab_size=8192, special_tokens=[\"UTT_BOUNDARY\", \"PAD\", \"UNK\"])\n",
    "tokenizer.train_from_iterator(list(dataset['text'])[:], trainer=trainer)\n",
    "\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"UTT_BOUNDARY $A\",\n",
    "    pair=\"UTT_BOUNDARY $A UTT_BOUNDARY $B:1\",\n",
    "    special_tokens=[(\"UTT_BOUNDARY\", tokenizer.token_to_id(\"UTT_BOUNDARY\"))],\n",
    ")\n",
    "\n",
    "wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer, pad_token='PAD', unk_token='UNK', bos_token='UTT_BOUNDARY', eos_token='UTT_BOUNDARY', add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: do you want to look at that it says look?\n",
      "Ids: [0, 5102, 1418, 2609, 2456, 416, 33]\n",
      "Tokens: ['UTT_BOUNDARY', 'doyouwantto', 'lookat', 'thatit', 'says', 'look', '?']\n",
      "Decoded: UTT_BOUNDARYdoyouwanttolookatthatitsayslook?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_example(dataset['text'][0], wrapped_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/transformersegmentation/BabyLM-BPE-ortho-tokenizer-spaceless/commit/dda79ed9ab5c2da9969623686600ab8fd2465b23', commit_message='Upload tokenizer', commit_description='', oid='dda79ed9ab5c2da9969623686600ab8fd2465b23', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer.push_to_hub(\"transformersegmentation/BabyLM-BPE-ortho-tokenizer-spaceless\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character-level tokenizer for orthographic text with/without word boundaries\n",
    "\n",
    "Since it's just a character-level model, the trainer can simply filter out the word boundary tokens so we do not need a separate tokenizer for no word boundaries. We train on the `character_split_utterance` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_COUNT = 100\n",
    "\n",
    "def build_vocabulary(lines):\n",
    "\n",
    "    vocab = {'UNK' : 0, 'PAD' : 1, 'WORD_BOUNDARY' : 2, 'UTT_BOUNDARY' : 3}\n",
    "    token_counts = {}\n",
    "    for line in lines:\n",
    "        tokens = line.strip().split()\n",
    "        for token in tokens:\n",
    "            if token not in token_counts:\n",
    "                token_counts[token] = 0\n",
    "            token_counts[token] += 1\n",
    "        \n",
    "    # Add tokens to vocab if they are not in phoible and have a count greater than MIN_COUNT\n",
    "    for token, count in token_counts.items():\n",
    "        if count > MIN_COUNT and token not in vocab:\n",
    "            vocab[token] = len(vocab)\n",
    "\n",
    "    print('Vocab: ', vocab)\n",
    "    print('Vocab size: ', len(vocab))\n",
    "    return vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'd': 4, 'o': 5, 'y': 6, 'u': 7, 'w': 8, 'a': 9, 'n': 10, 't': 11, 'l': 12, 'k': 13, 'h': 14, 'i': 15, 's': 16, '?': 17, 'e': 18, '.': 19, 'r': 20, \"'\": 21, 'f': 22, 'c': 23, 'g': 24, 'p': 25, 'b': 26, 'm': 27, 'v': 28, 'j': 29, '!': 30, 'x': 31, 'q': 32, 'z': 33, '-': 34, '&': 35, ',': 36, '/': 37, '1': 38, '9': 39, '5': 40, '0': 41, ';': 42, '‘': 43, '’': 44, '—': 45, ':': 46, '+': 47, '8': 48, '3': 49, '7': 50, '4': 51, '6': 52, '2': 53, '=': 54, ')': 55, '(': 56, '_': 57, '*': 58, '£': 59, '–': 60, '#': 61, '`': 62, '\"': 63, 'æ': 64, ']': 65, '|': 66, '$': 67, '“': 68, '”': 69, '[': 70, 'œ': 71, '{': 72, '}': 73, '…': 74, '°': 75, '§': 76, '>': 77, '·': 78, '¢': 79, '%': 80, '^': 81, '½': 82, '¶': 83, '×': 84, '¼': 85, '¾': 86, 'φ': 87, '<': 88, '´': 89, '¯': 90, '¦': 91, '\\x86': 92, '«': 93, '¬': 94, '©': 95, '\\x93': 96, '●': 97, '¹': 98, '⁄': 99, '²': 100, '₂': 101, '│': 102, 'δ': 103, 'α': 104, 'ο': 105, 'υ': 106, 'χ': 107, 'ι': 108, 'η': 109, 'μ': 110, 'ε': 111, 'γ': 112, 'λ': 113, 'β': 114, 'κ': 115, 'ω': 116, 'ν': 117, 'ρ': 118, 'σ': 119, 'ς': 120, 'τ': 121, 'θ': 122, 'π': 123, '′': 124, '³': 125, '⸺': 126, '―': 127, '±': 128, '~': 129, 'ß': 130, 'µ': 131, '„': 132, '@': 133, '\\\\': 134, '♪': 135, '¿': 136, '¡': 137, '\\u200b': 138, '─': 139, '\\x96': 140, 'ð': 141, '\\xad': 142, 'º': 143, '¸': 144, 'þ': 145, 'и': 146, '♫': 147, '¤': 148, '¨': 149, 'ø': 150, '\\ufeff': 151, 'ª': 152, 'đ': 153, '\\x80': 154, '\\x99': 155, '€': 156, 'ﬂ': 157, '♬': 158, '\\x92': 159, '，': 160, '\\x9d': 161, '™': 162, '®': 163, 'ı': 164, 'с': 165, 'в': 166, '\\u200e': 167, 'ц': 168, 'ь': 169, 'д': 170, 'н': 171, 'е': 172, 'м': 173, 'о': 174, 'т': 175, 'г': 176, 'а': 177, 'р': 178, 'ч': 179, 'п': 180, 'я': 181, 'б': 182, '•': 183, 'ł': 184, 'ร': 185, 'ก': 186, 'к': 187, 'у': 188, '\\x82': 189, '\\x88': 190, '\\x84': 191, '\\x83': 192, '\\x87': 193, '\\x8a': 194, '\\x91': 195, '大': 196, '\\x94': 197, 'ا': 198, 'л': 199, 'з': 200, '‑': 201, 'ʻ': 202, 'ت': 203, 'ו': 204, 'י': 205, 'ر': 206, 'ي': 207, 'ل': 208, 'ه': 209, 'ع': 210, 'ن': 211, 'ə': 212, '−': 213, '→': 214, 'ы': 215, 'ː': 216, 'و': 217, 'م': 218, 'س': 219, 'ح': 220, 'د': 221, 'ب': 222, 'ی': 223, '»': 224, 'น': 225, 'า': 226, 'ง': 227, 'ม': 228, 'ʼ': 229, 'ˈ': 230}\n",
      "Vocab size:  231\n"
     ]
    }
   ],
   "source": [
    "normalizer = normalizers.Sequence(\n",
    "        [normalizers.NFD(),\n",
    "         normalizers.Lowercase(),\n",
    "         normalizers.Strip(),\n",
    "         normalizers.StripAccents(),\n",
    "         normalizers.Replace(\"word_boundary\", \"WORD_BOUNDARY\"), # fix lowercasing for word boundary token\n",
    "        ]\n",
    "    )\n",
    "\n",
    "# We normalzie the text using the normalizer before building the vocabulary\n",
    "vocab = build_vocabulary([normalizer.normalize_str(line) for line in dataset['character_split_utterance']])\n",
    "\n",
    "tokenizer = Tokenizer(models.WordLevel(vocab=vocab, unk_token='UNK'))\n",
    "tokenizer.normalizer = normalizer\n",
    "tokenizer.add_special_tokens([\"UNK\", \"PAD\", \"UTT_BOUNDARY\", \"WORD_BOUNDARY\"])\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"UTT_BOUNDARY $A\",\n",
    "    pair=\"UTT_BOUNDARY $A UTT_BOUNDARY $B:1\",\n",
    "    special_tokens=[(\"UTT_BOUNDARY\", tokenizer.token_to_id(\"UTT_BOUNDARY\"))],\n",
    ")\n",
    "\n",
    "wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer, bos_token='UTT_BOUNDARY', eos_token='UTT_BOUNDARY', pad_token='PAD', unk_token='UNK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: d o WORD_BOUNDARY y o u WORD_BOUNDARY w a n t WORD_BOUNDARY t o WORD_BOUNDARY l o o k WORD_BOUNDARY a t WORD_BOUNDARY t h a t WORD_BOUNDARY i t WORD_BOUNDARY s a y s WORD_BOUNDARY l o o k ? WORD_BOUNDARY\n",
      "Ids: [3, 4, 5, 2, 6, 5, 7, 2, 8, 9, 10, 11, 2, 11, 5, 2, 12, 5, 5, 13, 2, 9, 11, 2, 11, 14, 9, 11, 2, 15, 11, 2, 16, 9, 6, 16, 2, 12, 5, 5, 13, 17, 2]\n",
      "Tokens: ['UTT_BOUNDARY', 'd', 'o', 'WORD_BOUNDARY', 'y', 'o', 'u', 'WORD_BOUNDARY', 'w', 'a', 'n', 't', 'WORD_BOUNDARY', 't', 'o', 'WORD_BOUNDARY', 'l', 'o', 'o', 'k', 'WORD_BOUNDARY', 'a', 't', 'WORD_BOUNDARY', 't', 'h', 'a', 't', 'WORD_BOUNDARY', 'i', 't', 'WORD_BOUNDARY', 's', 'a', 'y', 's', 'WORD_BOUNDARY', 'l', 'o', 'o', 'k', '?', 'WORD_BOUNDARY']\n",
      "Decoded: UTT_BOUNDARY d o WORD_BOUNDARY y o u WORD_BOUNDARY w a n t WORD_BOUNDARY t o WORD_BOUNDARY l o o k WORD_BOUNDARY a t WORD_BOUNDARY t h a t WORD_BOUNDARY i t WORD_BOUNDARY s a y s WORD_BOUNDARY l o o k? WORD_BOUNDARY\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_example(dataset['character_split_utterance'][0], wrapped_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab2 = vocab.copy()\n",
    "vocab2['W'] = vocab2.pop('WORD_BOUNDARY')\n",
    "\n",
    "tokenizer = Tokenizer(models.WordLevel(vocab=vocab2, unk_token='UNK'))\n",
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "        [normalizers.NFD(),\n",
    "         normalizers.Lowercase(),\n",
    "         normalizers.Strip(),\n",
    "         normalizers.StripAccents(),\n",
    "         normalizers.Replace(\" \", \"W\"),\n",
    "        ]\n",
    "    )\n",
    "tokenizer.add_special_tokens([\"UNK\", \"PAD\", \"UTT_BOUNDARY\", \"W\"])\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Split(\"\", behavior=\"isolated\")\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"UTT_BOUNDARY $A\",\n",
    "    pair=\"UTT_BOUNDARY $A UTT_BOUNDARY $B:1\",\n",
    "    special_tokens=[(\"UTT_BOUNDARY\", tokenizer.token_to_id(\"UTT_BOUNDARY\"))],\n",
    ")\n",
    "\n",
    "wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer, bos_token='UTT_BOUNDARY', eos_token='UTT_BOUNDARY', pad_token='PAD', unk_token='UNK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: do you want to look at that it says look?\n",
      "Ids: [3, 4, 5, 2, 6, 5, 7, 2, 8, 9, 10, 11, 2, 11, 5, 2, 12, 5, 5, 13, 2, 9, 11, 2, 11, 14, 9, 11, 2, 15, 11, 2, 16, 9, 6, 16, 2, 12, 5, 5, 13, 17]\n",
      "Tokens: ['UTT_BOUNDARY', 'd', 'o', 'W', 'y', 'o', 'u', 'W', 'w', 'a', 'n', 't', 'W', 't', 'o', 'W', 'l', 'o', 'o', 'k', 'W', 'a', 't', 'W', 't', 'h', 'a', 't', 'W', 'i', 't', 'W', 's', 'a', 'y', 's', 'W', 'l', 'o', 'o', 'k', '?']\n",
      "Decoded: UTT_BOUNDARY d o W y o u W w a n t W t o W l o o k W a t W t h a t W i t W s a y s W l o o k?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_example(dataset['text'][0], wrapped_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/transformersegmentation/BabyLM-char-tokenizer/commit/c98c9640822dce4314f7a84759a217091ac28658', commit_message='Upload tokenizer', commit_description='', oid='c98c9640822dce4314f7a84759a217091ac28658', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer.push_to_hub(\"transformersegmentation/BabyLM-char-tokenizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.WordLevel(vocab=vocab2, unk_token='UNK'))\n",
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "        [normalizers.NFD(),\n",
    "         normalizers.Lowercase(),\n",
    "         normalizers.Strip(),\n",
    "         normalizers.StripAccents(),\n",
    "         normalizers.Replace(\" \", \"\"), # Remove word boundaries\n",
    "        ]\n",
    "    )\n",
    "tokenizer.add_special_tokens([\"UNK\", \"PAD\", \"UTT_BOUNDARY\"])\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Split(\"\", behavior=\"isolated\")\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"UTT_BOUNDARY $A\",\n",
    "    pair=\"UTT_BOUNDARY $A UTT_BOUNDARY $B:1\",\n",
    "    special_tokens=[(\"UTT_BOUNDARY\", tokenizer.token_to_id(\"UTT_BOUNDARY\"))],\n",
    ")\n",
    "\n",
    "wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer, bos_token='UTT_BOUNDARY', eos_token='UTT_BOUNDARY', pad_token='PAD', unk_token='UNK')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: do you want to look at that it says look?\n",
      "Ids: [3, 4, 5, 6, 5, 7, 8, 9, 10, 11, 11, 5, 12, 5, 5, 13, 9, 11, 11, 14, 9, 11, 15, 11, 16, 9, 6, 16, 12, 5, 5, 13, 17]\n",
      "Tokens: ['UTT_BOUNDARY', 'd', 'o', 'y', 'o', 'u', 'w', 'a', 'n', 't', 't', 'o', 'l', 'o', 'o', 'k', 'a', 't', 't', 'h', 'a', 't', 'i', 't', 's', 'a', 'y', 's', 'l', 'o', 'o', 'k', '?']\n",
      "Decoded: UTT_BOUNDARY d o y o u w a n t t o l o o k a t t h a t i t s a y s l o o k?\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_example(dataset['text'][0], wrapped_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/transformersegmentation/BabyLM-char-tokenizer-spaceless/commit/0c523f1f927b6fa433ee4fc557852ff7ed42f29f', commit_message='Upload tokenizer', commit_description='', oid='0c523f1f927b6fa433ee4fc557852ff7ed42f29f', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer.push_to_hub(\"transformersegmentation/BabyLM-char-tokenizer-spaceless\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPE tokenizer for phonemes\n",
    "\n",
    "The phoneme data is space-separated by phoneme, with \"WORD_BOUNDARY\" separating words. We can use the normalizer to turn this back into word-like units for comparison with BPE on orthographic text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "        [normalizers.Replace(\" \", \"\"),\n",
    "         normalizers.Replace(\"WORD_BOUNDARY\", \" \"),\n",
    "         normalizers.Strip(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=True)\n",
    "trainer = trainers.BpeTrainer(vocab_size=8192, special_tokens=[\"UTT_BOUNDARY\", \"PAD\", \"UNK\"])\n",
    "tokenizer.train_from_iterator(dataset['phonemized_utterance'], trainer=trainer)\n",
    "\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"UTT_BOUNDARY $A\",\n",
    "    pair=\"UTT_BOUNDARY $A UTT_BOUNDARY $B:1\",\n",
    "    special_tokens=[(\"UTT_BOUNDARY\", tokenizer.token_to_id(\"UTT_BOUNDARY\"))],\n",
    ")\n",
    "\n",
    "wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer, pad_token='PAD', unk_token='UNK', bos_token='UTT_BOUNDARY', eos_token='UTT_BOUNDARY', add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: d uː WORD_BOUNDARY j uː WORD_BOUNDARY w ɔ n t WORD_BOUNDARY t ə WORD_BOUNDARY l ʊ k WORD_BOUNDARY æ t WORD_BOUNDARY ð ʌ t ɪ t WORD_BOUNDARY s ɛ z WORD_BOUNDARY l ʊ k WORD_BOUNDARY\n",
      "Ids: [0, 199, 111, 341, 119, 406, 215, 1616, 1137, 406]\n",
      "Tokens: ['UTT_BOUNDARY', 'ĠduËĲ', 'ĠjuËĲ', 'ĠwÉĶnt', 'ĠtÉĻ', 'ĠlÊĬk', 'ĠÃ¦t', 'ĠÃ°ÊĮtÉªt', 'ĠsÉĽz', 'ĠlÊĬk']\n",
      "Decoded: UTT_BOUNDARY duː juː wɔnt tə lʊk æt ðʌtɪt sɛz lʊk\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_example(dataset['phonemized_utterance'][0], wrapped_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/transformersegmentation/BabyLM-BPE-phoneme-tokenizer/commit/29eb69a8ad35547189fb583dc055dacdbe9b3fb7', commit_message='Upload tokenizer', commit_description='', oid='29eb69a8ad35547189fb583dc055dacdbe9b3fb7', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer.push_to_hub(\"transformersegmentation/BabyLM-BPE-phoneme-tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BPE tokenizer for phonemes without spaces\n",
    "\n",
    "Similar to the BPE for orthographic text. The only difference is the normalizer and the fact we train on phonemes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(models.BPE())\n",
    "tokenizer.normalizer = normalizers.Sequence(\n",
    "        [normalizers.Replace(\" \", \"\"),\n",
    "         normalizers.Replace(\"WORD_BOUNDARY\", \"\"), # Remove word boundaries\n",
    "        #  normalizers.Replace(\"t̠ʃ\", 'S'),\n",
    "        #  normalizers.Replace(\"tʰ\", 'T'),\n",
    "        #  normalizers.Replace(\"uː\", 'U'),\n",
    "        #  normalizers.Replace(\"ɪ\", 'I'),\n",
    "        #  normalizers.Replace(\"ɜː\", 'E'),\n",
    "        #  normalizers.Replace(\"ɛː\", '1'),\n",
    "        #  normalizers.Replace(\"ə\", 'F'),\n",
    "        #  normalizers.Replace(\"əʊ\", 'G'),\n",
    "        #  normalizers.Replace(\"æ\", 'A'),\n",
    "        #  normalizers.Replace(\"ɔ\", 'O'),\n",
    "        #  normalizers.Replace(\"ʌ\", 'V'),\n",
    "        #  normalizers.Replace(\"aʊ\", 'X'),\n",
    "        #  normalizers.Replace(\"ɔɪ\", 'Y'),\n",
    "        #  normalizers.Replace(\"ʊ\", 'Z'),\n",
    "        #  normalizers.Replace(\"ɯ\", 'W'),\n",
    "        #  normalizers.Replace(\"eə\", 'Q'),\n",
    "        #  normalizers.Replace(\"ç\", 'C'),\n",
    "        #  normalizers.Replace(\"r̩\", 'R'),\n",
    "        #  normalizers.Replace(\"nʲ\", 'N'),\n",
    "        #  normalizers.Replace(\"tɕ\", 'J'),\n",
    "        #  normalizers.Replace(\"ɬ\", 'L'),\n",
    "        #  normalizers.Replace(\"aɪ\", '2'),\n",
    "        #  normalizers.Replace(\"ð\", 'H'),\n",
    "        #  normalizers.Replace(\"ʃ\", '3'),\n",
    "        #  normalizers.Replace(\"ɪː\", '4'),\n",
    "        #  normalizers.Replace(\"ɑ̃\", '5'),\n",
    "        #  normalizers.Replace(\"ʒ\", '6'),\n",
    "        #  normalizers.Replace(\"ɒ\", '7'),\n",
    "        #  normalizers.Replace(\"ŋ\", '8'),\n",
    "        #  normalizers.Replace(\"ɛ\", 'D'),\n",
    "        #  normalizers.Replace(\"eɪ\", '9'),\n",
    "        #  normalizers.Replace(\"ɹ\", 'K'),\n",
    "        #  normalizers.Replace(\"oʊ\", '0'),\n",
    "        #  normalizers.Replace(\"θ\", 'B'),\n",
    "        #  normalizers.Replace(\"iː\", 'P'),\n",
    "        #  normalizers.Replace(\"d̠ʒ\", 'M'),\n",
    "        #  normalizers.Replace(\"iə\", ':'),\n",
    "         normalizers.Strip(),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.ByteLevel(add_prefix_space=False, use_regex=True)\n",
    "trainer = trainers.BpeTrainer(vocab_size=8192, special_tokens=[\"UTT_BOUNDARY\", \"PAD\", \"UNK\"])\n",
    "tokenizer.train_from_iterator(dataset['phonemized_utterance'], trainer=trainer)\n",
    "\n",
    "tokenizer.decoder = decoders.ByteLevel()\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"UTT_BOUNDARY $A\",\n",
    "    pair=\"UTT_BOUNDARY $A UTT_BOUNDARY $B:1\",\n",
    "    special_tokens=[(\"UTT_BOUNDARY\", tokenizer.token_to_id(\"UTT_BOUNDARY\"))],\n",
    ")\n",
    "\n",
    "wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer, pad_token='PAD', unk_token='UNK', bos_token='UTT_BOUNDARY', eos_token='UTT_BOUNDARY', add_prefix_space=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: d uː WORD_BOUNDARY j uː WORD_BOUNDARY w ɔ n t WORD_BOUNDARY t ə WORD_BOUNDARY l ʊ k WORD_BOUNDARY æ t WORD_BOUNDARY ð ʌ t ɪ t WORD_BOUNDARY s ɛ z WORD_BOUNDARY l ʊ k WORD_BOUNDARY\n",
      "Ids: [0, 3207, 3199, 101, 2556, 1175, 419]\n",
      "Tokens: ['UTT_BOUNDARY', 'duËĲjuËĲwÉĶnt', 'tÉĻlÊĬk', 'Ã¦t', 'Ã°ÊĮtÉªt', 'sÉĽz', 'lÊĬk']\n",
      "Decoded: UTT_BOUNDARYduːjuːwɔnttəlʊkætðʌtɪtsɛzlʊk\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_example(dataset['phonemized_utterance'][0], wrapped_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/transformersegmentation/BabyLM-BPE-phoneme-tokenizer-spaceless/commit/4608f0eb4e14a16b6bfa2896e97bcdd2428348fb', commit_message='Upload tokenizer', commit_description='', oid='4608f0eb4e14a16b6bfa2896e97bcdd2428348fb', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer.push_to_hub(\"transformersegmentation/BabyLM-BPE-phoneme-tokenizer-spaceless\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u0007\n"
     ]
    }
   ],
   "source": [
    "# Play a ding sound\n",
    "print(\"\\a\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phoneme tokenizer with spaces\n",
    "\n",
    "Character-based tokenizer that just uses the phonemes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nz/6tzh0bsj2txd1cz18gpcms_c0000gn/T/ipykernel_32991/1543845252.py:1: DtypeWarning: Columns (4,7,8,11) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  phoible = pd.read_csv('../../data/phoible.csv')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens not found in phoible:  {'1': 690, 'kh': 17638, 'ʌʌ': 308}\n",
      "Vocab:  {'UNK': 0, 'PAD': 1, 'WORD_BOUNDARY': 2, 'UTT_BOUNDARY': 3, 'd': 4, 'uː': 5, 'j': 6, 'w': 7, 'ɔ': 8, 'n': 9, 't': 10, 'ə': 11, 'l': 12, 'ʊ': 13, 'k': 14, 'æ': 15, 'ð': 16, 'ʌ': 17, 'ɪ': 18, 's': 19, 'ɛ': 20, 'z': 21, 'iː': 22, 'ɹ': 23, 'f': 24, 'eɪ': 25, 'ɡ': 26, 'ɑ': 27, 'h': 28, 'p': 29, 'b': 30, 'i': 31, 't̠ʃ': 32, 'aɪ': 33, 'θ': 34, 'ŋ': 35, 'm': 36, 'ɔɪ': 37, 'oʊ': 38, 'aʊ': 39, 'v': 40, 'ɜː': 41, 'd̠ʒ': 42, 'ʃ': 43, 'iə': 44, 'ʒ': 45, 'ɑ̃': 46, 'r': 47, 'nʲ': 48, 'x': 49, 'ɬ': 50, 'ç': 51, 'e': 52, 'o': 53, 'ɛː': 54, 'ɪː': 55, 'u': 56, 'q': 57, 'tɕ': 58, 'tʰ': 59, 'ɯ': 60, 'r̩': 61, 'əʊ': 62, 'a': 63, 'ɒ': 64, 'eə': 65}\n",
      "Vocab size:  66\n"
     ]
    }
   ],
   "source": [
    "phoible = pd.read_csv('../../data/phoible.csv')\n",
    "phoible_phonemes = phoible.Phoneme.unique()\n",
    "\n",
    "vocab = {'UNK' : 0, 'PAD' : 1, 'WORD_BOUNDARY' : 2, 'UTT_BOUNDARY' : 3}\n",
    "unk_tokens = []\n",
    "token_counts = {}\n",
    "for line in dataset['phonemized_utterance']:\n",
    "    tokens = line.strip().split()\n",
    "    for token in tokens:\n",
    "        if token not in token_counts:\n",
    "            token_counts[token] = 0\n",
    "        token_counts[token] += 1\n",
    "    \n",
    "# Add tokens to vocab if they are not in phoible and have a count greater than MIN_COUNT\n",
    "for token, count in token_counts.items():\n",
    "    if count > MIN_COUNT and token not in vocab:\n",
    "        if token not in phoible_phonemes:\n",
    "            unk_tokens.append(token)\n",
    "        else:\n",
    "            vocab[token] = len(vocab)\n",
    "\n",
    "print('Tokens not found in phoible: ', {token: token_counts[token] for token in unk_tokens})\n",
    "print('Vocab: ', vocab)\n",
    "print('Vocab size: ', len(vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.WordLevel(vocab=vocab, unk_token='UNK'))\n",
    "tokenizer.normalizer = normalizers.Sequence([normalizers.Strip()]) \n",
    "tokenizer.add_special_tokens([\"UNK\", \"PAD\", \"WORD_BOUNDARY\", \"UTT_BOUNDARY\"])\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"UTT_BOUNDARY $A\",\n",
    "    pair=\"UTT_BOUNDARY $A UTT_BOUNDARY $B:1\",\n",
    "    special_tokens=[(\"UTT_BOUNDARY\", tokenizer.token_to_id(\"UTT_BOUNDARY\"))],\n",
    ")\n",
    "\n",
    "wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer, bos_token='UTT_BOUNDARY', eos_token='UTT_BOUNDARY', pad_token='PAD', unk_token='UNK')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: d uː WORD_BOUNDARY j uː WORD_BOUNDARY w ɔ n t WORD_BOUNDARY t ə WORD_BOUNDARY l ʊ k WORD_BOUNDARY æ t WORD_BOUNDARY ð ʌ t ɪ t WORD_BOUNDARY s ɛ z WORD_BOUNDARY l ʊ k WORD_BOUNDARY\n",
      "Ids: [3, 4, 5, 2, 6, 5, 2, 7, 8, 9, 10, 2, 10, 11, 2, 12, 13, 14, 2, 15, 10, 2, 16, 17, 10, 18, 10, 2, 19, 20, 21, 2, 12, 13, 14, 2]\n",
      "Tokens: ['UTT_BOUNDARY', 'd', 'uː', 'WORD_BOUNDARY', 'j', 'uː', 'WORD_BOUNDARY', 'w', 'ɔ', 'n', 't', 'WORD_BOUNDARY', 't', 'ə', 'WORD_BOUNDARY', 'l', 'ʊ', 'k', 'WORD_BOUNDARY', 'æ', 't', 'WORD_BOUNDARY', 'ð', 'ʌ', 't', 'ɪ', 't', 'WORD_BOUNDARY', 's', 'ɛ', 'z', 'WORD_BOUNDARY', 'l', 'ʊ', 'k', 'WORD_BOUNDARY']\n",
      "Decoded: UTT_BOUNDARY d uː WORD_BOUNDARY j uː WORD_BOUNDARY w ɔ n t WORD_BOUNDARY t ə WORD_BOUNDARY l ʊ k WORD_BOUNDARY æ t WORD_BOUNDARY ð ʌ t ɪ t WORD_BOUNDARY s ɛ z WORD_BOUNDARY l ʊ k WORD_BOUNDARY\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_example(dataset['phonemized_utterance'][0], wrapped_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/transformersegmentation/BabyLM-phoneme-tokenizer/commit/05288fcf78055b38245bfe8d807060d4f01568b5', commit_message='Upload tokenizer', commit_description='', oid='05288fcf78055b38245bfe8d807060d4f01568b5', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer.push_to_hub(\"transformersegmentation/BabyLM-phoneme-tokenizer\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phoneme tokenizer without spaces\n",
    "\n",
    "Character-based tokenizer that uses the phonemes and removes word boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(models.WordLevel(vocab=vocab, unk_token='UNK'))\n",
    "tokenizer.normalizer = normalizers.Sequence([normalizers.Replace(' WORD_BOUNDARY', ''), normalizers.Strip()]) \n",
    "tokenizer.add_special_tokens([\"UNK\", \"PAD\", \"UTT_BOUNDARY\"])\n",
    "tokenizer.pre_tokenizer = pre_tokenizers.Whitespace()\n",
    "tokenizer.post_processor = processors.TemplateProcessing(\n",
    "    single=\"UTT_BOUNDARY $A\",\n",
    "    pair=\"UTT_BOUNDARY $A UTT_BOUNDARY $B:1\",\n",
    "    special_tokens=[(\"UTT_BOUNDARY\", tokenizer.token_to_id(\"UTT_BOUNDARY\"))],\n",
    ")\n",
    "\n",
    "wrapped_tokenizer = GPT2TokenizerFast(tokenizer_object=tokenizer, bos_token='UTT_BOUNDARY', eos_token='UTT_BOUNDARY', pad_token='PAD', unk_token='UNK')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: d uː WORD_BOUNDARY j uː WORD_BOUNDARY w ɔ n t WORD_BOUNDARY t ə WORD_BOUNDARY l ʊ k WORD_BOUNDARY æ t WORD_BOUNDARY ð ʌ t ɪ t WORD_BOUNDARY s ɛ z WORD_BOUNDARY l ʊ k WORD_BOUNDARY\n",
      "Ids: [3, 4, 5, 6, 5, 7, 8, 9, 10, 10, 11, 12, 13, 14, 15, 10, 16, 17, 10, 18, 10, 19, 20, 21, 12, 13, 14]\n",
      "Tokens: ['UTT_BOUNDARY', 'd', 'uː', 'j', 'uː', 'w', 'ɔ', 'n', 't', 't', 'ə', 'l', 'ʊ', 'k', 'æ', 't', 'ð', 'ʌ', 't', 'ɪ', 't', 's', 'ɛ', 'z', 'l', 'ʊ', 'k']\n",
      "Decoded: UTT_BOUNDARY d uː j uː w ɔ n t t ə l ʊ k æ t ð ʌ t ɪ t s ɛ z l ʊ k\n",
      "\n"
     ]
    }
   ],
   "source": [
    "show_example(dataset['phonemized_utterance'][0], wrapped_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/transformersegmentation/BabyLM-phoneme-tokenizer-spaceless/commit/d0f7972bc48b28fa2daa74bf92c4083a9d3acabd', commit_message='Upload tokenizer', commit_description='', oid='d0f7972bc48b28fa2daa74bf92c4083a9d3acabd', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wrapped_tokenizer.push_to_hub(\"transformersegmentation/BabyLM-phoneme-tokenizer-spaceless\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare all tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "transformersegmentation/BabyLM-BPE-ortho-tokenizer\n",
      "8192\n",
      "Original: what a conundrum !\n",
      "Ids: [0, 285, 181, 354, 1727, 58, 355, 2862]\n",
      "Tokens: ['UTT_BOUNDARY', 'Ġwhat', 'Ġa', 'Ġcon', 'und', 'r', 'um', 'Ġ!']\n",
      "Decoded: UTT_BOUNDARY what a conundrum!\n",
      "\n",
      "transformersegmentation/BabyLM-BPE-ortho-tokenizer-spaceless\n",
      "8192\n",
      "Original: what a conundrum !\n",
      "Ids: [0, 264, 1981, 237, 6822, 3]\n",
      "Tokens: ['UTT_BOUNDARY', 'what', 'acon', 'un', 'drum', '!']\n",
      "Decoded: UTT_BOUNDARYwhataconundrum!\n",
      "\n",
      "transformersegmentation/BabyLM-char-tokenizer\n",
      "231\n",
      "Original: what a conundrum !\n",
      "Ids: [3, 8, 14, 9, 11, 2, 9, 2, 23, 5, 10, 7, 10, 4, 20, 7, 27, 2, 30]\n",
      "Tokens: ['UTT_BOUNDARY', 'w', 'h', 'a', 't', 'W', 'a', 'W', 'c', 'o', 'n', 'u', 'n', 'd', 'r', 'u', 'm', 'W', '!']\n",
      "Decoded: UTT_BOUNDARY w h a t W a W c o n u n d r u m W!\n",
      "\n",
      "transformersegmentation/BabyLM-char-tokenizer-spaceless\n",
      "231\n",
      "Original: what a conundrum !\n",
      "Ids: [3, 8, 14, 9, 11, 9, 23, 5, 10, 7, 10, 4, 20, 7, 27, 30]\n",
      "Tokens: ['UTT_BOUNDARY', 'w', 'h', 'a', 't', 'a', 'c', 'o', 'n', 'u', 'n', 'd', 'r', 'u', 'm', '!']\n",
      "Decoded: UTT_BOUNDARY w h a t a c o n u n d r u m!\n",
      "\n",
      "transformersegmentation/BabyLM-BPE-phoneme-tokenizer\n",
      "8192\n",
      "Original: w ʌ t WORD_BOUNDARY ʌ WORD_BOUNDARY k ə n ʌ n d ɹ ə m WORD_BOUNDARY\n",
      "Ids: [0, 180, 73, 668, 177, 6143]\n",
      "Tokens: ['UTT_BOUNDARY', 'ĠwÊĮt', 'ĠÊĮ', 'ĠkÉĻn', 'ÊĮnd', 'É¹ÉĻm']\n",
      "Decoded: UTT_BOUNDARY wʌt ʌ kənʌndɹəm\n",
      "\n",
      "transformersegmentation/BabyLM-BPE-phoneme-tokenizer-spaceless\n",
      "8192\n",
      "Original: w ʌ t WORD_BOUNDARY ʌ WORD_BOUNDARY k ə n ʌ n d ɹ ə m WORD_BOUNDARY\n",
      "Ids: [0, 156, 7883, 314, 5551]\n",
      "Tokens: ['UTT_BOUNDARY', 'wÊĮt', 'ÊĮkÉĻn', 'ÊĮnd', 'É¹ÉĻm']\n",
      "Decoded: UTT_BOUNDARYwʌtʌkənʌndɹəm\n",
      "\n",
      "transformersegmentation/BabyLM-phoneme-tokenizer\n",
      "66\n",
      "Original: w ʌ t WORD_BOUNDARY ʌ WORD_BOUNDARY k ə n ʌ n d ɹ ə m WORD_BOUNDARY\n",
      "Ids: [3, 7, 17, 10, 2, 17, 2, 14, 11, 9, 17, 9, 4, 23, 11, 36, 2]\n",
      "Tokens: ['UTT_BOUNDARY', 'w', 'ʌ', 't', 'WORD_BOUNDARY', 'ʌ', 'WORD_BOUNDARY', 'k', 'ə', 'n', 'ʌ', 'n', 'd', 'ɹ', 'ə', 'm', 'WORD_BOUNDARY']\n",
      "Decoded: UTT_BOUNDARY w ʌ t WORD_BOUNDARY ʌ WORD_BOUNDARY k ə n ʌ n d ɹ ə m WORD_BOUNDARY\n",
      "\n",
      "transformersegmentation/BabyLM-phoneme-tokenizer-spaceless\n",
      "66\n",
      "Original: w ʌ t WORD_BOUNDARY ʌ WORD_BOUNDARY k ə n ʌ n d ɹ ə m WORD_BOUNDARY\n",
      "Ids: [3, 7, 17, 10, 17, 14, 11, 9, 17, 9, 4, 23, 11, 36]\n",
      "Tokens: ['UTT_BOUNDARY', 'w', 'ʌ', 't', 'ʌ', 'k', 'ə', 'n', 'ʌ', 'n', 'd', 'ɹ', 'ə', 'm']\n",
      "Decoded: UTT_BOUNDARY w ʌ t ʌ k ə n ʌ n d ɹ ə m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizers = [\n",
    "    \"babylm/babyllama-100m-2024\",\n",
    "    \"babylm/ltgbert-100m-2024\",\n",
    "    \"transformersegmentation/BabyLM-BPE-ortho-tokenizer\",\n",
    "    \"transformersegmentation/BabyLM-BPE-ortho-tokenizer\",\n",
    "    \"transformersegmentation/BabyLM-BPE-ortho-tokenizer-spaceless\",\n",
    "    \"transformersegmentation/BabyLM-char-tokenizer\",\n",
    "    \"transformersegmentation/BabyLM-char-tokenizer-spaceless\",\n",
    "    \"transformersegmentation/BabyLM-BPE-phoneme-tokenizer\",\n",
    "    \"transformersegmentation/BabyLM-BPE-phoneme-tokenizer-spaceless\",\n",
    "    \"transformersegmentation/BabyLM-phoneme-tokenizer\",\n",
    "    \"transformersegmentation/BabyLM-phoneme-tokenizer-spaceless\",\n",
    "]\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "text_example = \"what a conundrum !\"\n",
    "phoneme_example = \"w ʌ t WORD_BOUNDARY ʌ WORD_BOUNDARY k ə n ʌ n d ɹ ə m WORD_BOUNDARY\"\n",
    "\n",
    "for tokenizer in tokenizers:\n",
    "    print(tokenizer)\n",
    "    t = AutoTokenizer.from_pretrained(tokenizer)    \n",
    "    print(len(t.get_vocab()))\n",
    "    show_example(phoneme_example if 'phoneme' in tokenizer else text_example, t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
