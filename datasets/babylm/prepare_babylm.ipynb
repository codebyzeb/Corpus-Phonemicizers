{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phonemize the BabyLM Dataset\n",
    "\n",
    "We produce a Huggingface dataset that contains the BabyLM dataset (with some cleaning applied) as well as phonemized versions of each line. We begin by loading the original dataset stored within the cambridge-climb version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zebulongoriely/Documents/UniDocs/PHD/research/projects/CHILDES_processor/env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/zebulongoriely/Documents/UniDocs/PHD/research/projects/CHILDES_processor/env/lib/python3.9/site-packages/datasets/load.py:1461: FutureWarning: The repository for cambridge-climb/BabyLM contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/cambridge-climb/BabyLM\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "/Users/zebulongoriely/Documents/UniDocs/PHD/research/projects/CHILDES_processor/env/lib/python3.9/site-packages/datasets/load.py:1461: FutureWarning: The repository for cambridge-climb/BabyLM contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/cambridge-climb/BabyLM\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import pandas as pd\n",
    "\n",
    "dataset_strict = datasets.load_dataset(\"cambridge-climb/BabyLM\", \"original_strict\")\n",
    "dataset_strict_small = datasets.load_dataset(\"cambridge-climb/BabyLM\", \"original_strict_small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'tagged_text', 'filename'],\n",
       "    num_rows: 1015485\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_strict_small['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "\n",
    "We apply some light cleaning to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¼ Cleaning 'strict_train'\n",
      "ðŸ§¹ Cleaned 'aochildes.txt' (size 763988 -> 763988)\n",
      "ðŸ§¹ Cleaned 'bnc_spoken.txt' (size 848199 -> 848199)\n",
      "ðŸ§¹ Cleaned 'cbt.txt' (size 263518 -> 263518)\n",
      "ðŸ§¹ Cleaned 'children_stories.txt' (size 76379 -> 76379)\n",
      "ðŸ§¹ Cleaned 'gutenberg.txt' (size 898292 -> 898292)\n",
      "ðŸ§¹ Cleaned 'open_subtitles.txt' (size 5433930 -> 5433127)\n",
      "ðŸ§¹ Cleaned 'qed.txt' (size 959619 -> 959844)\n",
      "ðŸ§¹ Cleaned 'simple_wikipedia.txt' (size 567001 -> 567001)\n",
      "ðŸ§¹ Cleaned 'switchboard.txt' (size 161739 -> 161739)\n",
      "ðŸ§¹ Cleaned 'wikipedia.txt' (size 203011 -> 177085)\n",
      "ðŸ§¼ Cleaning 'strict_small_train'\n",
      "ðŸ§¹ Cleaned 'aochildes.txt' (size 79999 -> 79999)\n",
      "ðŸ§¹ Cleaned 'bnc_spoken.txt' (size 89931 -> 89931)\n",
      "ðŸ§¹ Cleaned 'cbt.txt' (size 25999 -> 25999)\n",
      "ðŸ§¹ Cleaned 'children_stories.txt' (size 5731 -> 5731)\n",
      "ðŸ§¹ Cleaned 'gutenberg.txt' (size 94502 -> 94502)\n",
      "ðŸ§¹ Cleaned 'open_subtitles.txt' (size 527394 -> 527316)\n",
      "ðŸ§¹ Cleaned 'qed.txt' (size 99928 -> 99932)\n",
      "ðŸ§¹ Cleaned 'simple_wikipedia.txt' (size 56616 -> 56616)\n",
      "ðŸ§¹ Cleaned 'switchboard.txt' (size 15739 -> 15739)\n",
      "ðŸ§¹ Cleaned 'wikipedia.txt' (size 19615 -> 16930)\n",
      "ðŸ§¼ Cleaning 'strict_valid'\n",
      "ðŸ§¹ Cleaned 'aochildes.txt' (size 69999 -> 69999)\n",
      "ðŸ§¹ Cleaned 'bnc_spoken.txt' (size 89917 -> 89917)\n",
      "ðŸ§¹ Cleaned 'cbt.txt' (size 12746 -> 12746)\n",
      "ðŸ§¹ Cleaned 'children_stories.txt' (size 5989 -> 5989)\n",
      "ðŸ§¹ Cleaned 'gutenberg.txt' (size 80468 -> 80468)\n",
      "ðŸ§¹ Cleaned 'open_subtitles.txt' (size 529408 -> 529338)\n",
      "ðŸ§¹ Cleaned 'qed.txt' (size 94975 -> 94976)\n",
      "ðŸ§¹ Cleaned 'simple_wikipedia.txt' (size 60976 -> 60976)\n",
      "ðŸ§¹ Cleaned 'switchboard.txt' (size 17999 -> 17999)\n",
      "ðŸ§¹ Cleaned 'wikipedia.txt' (size 23477 -> 20236)\n",
      "ðŸ§¼ Cleaning 'strict_small_valid'\n",
      "ðŸ§¹ Cleaned 'aochildes.txt' (size 69999 -> 69999)\n",
      "ðŸ§¹ Cleaned 'bnc_spoken.txt' (size 89917 -> 89917)\n",
      "ðŸ§¹ Cleaned 'cbt.txt' (size 12746 -> 12746)\n",
      "ðŸ§¹ Cleaned 'children_stories.txt' (size 5989 -> 5989)\n",
      "ðŸ§¹ Cleaned 'gutenberg.txt' (size 80468 -> 80468)\n",
      "ðŸ§¹ Cleaned 'open_subtitles.txt' (size 529408 -> 529338)\n",
      "ðŸ§¹ Cleaned 'qed.txt' (size 94975 -> 94976)\n",
      "ðŸ§¹ Cleaned 'simple_wikipedia.txt' (size 60976 -> 60976)\n",
      "ðŸ§¹ Cleaned 'switchboard.txt' (size 17999 -> 17999)\n",
      "ðŸ§¹ Cleaned 'wikipedia.txt' (size 23477 -> 20236)\n"
     ]
    }
   ],
   "source": [
    "from cleaning import *\n",
    "\n",
    "CLEANUP_FUNCTIONS = {\n",
    "    'aochildes': cleanup_aochildes,\n",
    "    'bnc_spoken': cleanup_bnc_spoken,\n",
    "    'cbt': cleanup_cbt,\n",
    "    'children_stories': cleanup_children_stories,\n",
    "    'gutenberg': cleanup_gutenberg,\n",
    "    'open_subtitles': cleanup_open_subtitles,\n",
    "    'qed': cleanup_qed,\n",
    "    'simple_wikipedia': cleanup_simple_wikipedia,\n",
    "    'switchboard': cleanup_switchboard,\n",
    "    'wikipedia': cleanup_wikipedia,\n",
    "}\n",
    "\n",
    "def dataset_to_dataframe(dataset):\n",
    "    df = dataset.to_pandas()\n",
    "    df = df.drop(columns=['tagged_text'])\n",
    "    remove = ['None', 'nan', 'NaN']\n",
    "    # When saving the dataset, strings with 'None' or 'nan' or 'NaN'\n",
    "    # are converted to None values and this causes problems\n",
    "    df = df[~df['text'].isin(remove)]\n",
    "    return df\n",
    "\n",
    "def cleanup(df):\n",
    "    new_df = {'filename': [], 'text': []}\n",
    "    for filename in df['filename'].unique():\n",
    "        lines = list(df[df['filename'] == filename]['text'])\n",
    "        new_lines = CLEANUP_FUNCTIONS[filename.split('.')[0]]('\\n'.join(lines)).split('\\n')\n",
    "        new_lines = [new_line for new_line in new_lines if new_line.strip() != '']\n",
    "        new_df['filename'].extend([filename] * len(new_lines))\n",
    "        new_df['text'].extend(new_lines)\n",
    "        print(f\"ðŸ§¹ Cleaned '{filename}' (size {len(lines)} -> {len(new_lines)})\")\n",
    "    return pd.DataFrame(new_df)\n",
    "\n",
    "dfs = {\n",
    "    'strict_train': dataset_to_dataframe(dataset_strict['train']),\n",
    "    'strict_small_train': dataset_to_dataframe(dataset_strict_small['train']),\n",
    "    'strict_valid': dataset_to_dataframe(dataset_strict['validation']),\n",
    "    'strict_small_valid': dataset_to_dataframe(dataset_strict_small['validation']),\n",
    "}\n",
    "\n",
    "for name, df in dfs.items():\n",
    "    print(f\"ðŸ§¼ Cleaning '{name}'\")\n",
    "    dfs[name] = cleanup(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phonemicize\n",
    "\n",
    "Use our phonemicizer code to add a phonemic transcription of every line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”  Adding phonemes to 'strict_train'\n",
      "Phonemizing using language \"EnglishNA\"...\n",
      "Using espeak backend with language code \"en-us\"...\n",
      "ðŸ”  Added phonemes... (size 10149172 -> 10117701)\n",
      "ðŸ”  Adding phonemes to 'strict_small_train'\n",
      "Phonemizing using language \"EnglishNA\"...\n",
      "Using espeak backend with language code \"en-us\"...\n",
      "ðŸ”  Added phonemes... (size 1012695 -> 1009906)\n",
      "ðŸ”  Adding phonemes to 'strict_valid'\n",
      "Phonemizing using language \"EnglishNA\"...\n",
      "Using espeak backend with language code \"en-us\"...\n",
      "ðŸ”  Added phonemes... (size 982644 -> 979449)\n",
      "ðŸ”  Adding phonemes to 'strict_small_valid'\n",
      "Phonemizing using language \"EnglishNA\"...\n",
      "Using espeak backend with language code \"en-us\"...\n",
      "ðŸ”  Added phonemes... (size 982644 -> 979449)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ['PHONEMIZER_ESPEAK_LIBRARY'] = '/opt/local/lib/libespeak-ng.dylib'\n",
    "sys.path.append('../../')\n",
    "from src.phonemize import phonemize_utterances, character_split_utterance\n",
    "\n",
    "def add_phonemes(df):\n",
    "    lines = df['text'].tolist()\n",
    "    len_before = len(lines)\n",
    "    # The Espeak backend is used for phonemization but will sometimes place word boundaries in places\n",
    "    # that don't match the orthography. E.g. \"that's it\" might become one word instead of two. This is\n",
    "    # not so much a problem for our cases, unless we're interested in the word boundaries themselves.\n",
    "    phonemized = phonemize_utterances(lines, keep_word_boundaries=True, allow_possibly_faulty_word_boundaries=True)\n",
    "    df['phonemized_utterance'] = phonemized\n",
    "    # We also split the phonemized text into characters for the character-level model\n",
    "    df['character_split_utterance'] = character_split_utterance(lines)\n",
    "    df = df[df['phonemized_utterance'] != '']\n",
    "    len_after = len(df)\n",
    "    print(f\"ðŸ”  Added phonemes... (size {len_before} -> {len_after})\")\n",
    "\n",
    "for name, df in dfs.items():\n",
    "    print(f\"ðŸ”  Adding phonemes to '{name}'\")\n",
    "    add_phonemes(df) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to Huggingface Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in dfs.items():\n",
    "    # We don't need the entire validation set\n",
    "    if 'valid' in name:\n",
    "        df = df.sample(n=10000, random_state=42)\n",
    "    df.to_csv(f'BabyLM-phonemized/{name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test import of dataset\n",
    "\n",
    "The dataset is saved at `BabyLM-phonemized/`. We don't need to push it to Huggingface to load it here, we can provide a local path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 10144218 examples [00:47, 215606.90 examples/s]\n",
      "Generating valid split: 10000 examples [00:00, 220462.76 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset('BabyLM-phonemized', 'strict', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In computing, NaN stands for Not a Number. It means the result of input is or unrepresentable. For example, division by zero in most Programming language returns NaN. Systematic use of NaN was introduced by the IEEE 754 in 1985, along with the other numbers such as infinities.\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset['processed_gloss'][9240649])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, line in enumerate(train_dataset['processed_gloss']):\n",
    "    if line is None:\n",
    "        print('None found')\n",
    "        print(i)\n",
    "        print(line)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
