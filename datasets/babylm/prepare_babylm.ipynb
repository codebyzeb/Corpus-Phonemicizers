{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Phonemize the BabyLM Dataset\n",
    "\n",
    "We produce a Huggingface dataset that contains the BabyLM dataset (with some cleaning applied) as well as phonemized versions of each line. We begin by loading the original dataset stored within the cambridge-climb version:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zebulongoriely/Documents/UniDocs/PHD/research/projects/CHILDES_processor/env/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/Users/zebulongoriely/Documents/UniDocs/PHD/research/projects/CHILDES_processor/env/lib/python3.9/site-packages/datasets/load.py:1461: FutureWarning: The repository for cambridge-climb/BabyLM contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/cambridge-climb/BabyLM\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Downloading builder script: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13.7k/13.7k [00:00<00:00, 4.51MB/s]\n",
      "Downloading readme: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.73k/4.73k [00:00<00:00, 3.18MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 99.8M/99.8M [00:18<00:00, 5.31MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 225M/225M [00:41<00:00, 5.36MB/s] \n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 133M/133M [00:24<00:00, 5.41MB/s] \n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90.7M/90.7M [00:16<00:00, 5.34MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 274M/274M [00:52<00:00, 5.18MB/s] \n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 920M/920M [02:46<00:00, 5.51MB/s] \n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 292M/292M [00:50<00:00, 5.81MB/s] \n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 430M/430M [01:15<00:00, 5.68MB/s] \n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34.5M/34.5M [00:05<00:00, 5.81MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 295M/295M [00:50<00:00, 5.85MB/s] \n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.72M/9.72M [00:01<00:00, 5.99MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23.6M/23.6M [00:04<00:00, 5.62MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 6.13M/6.13M [00:01<00:00, 5.00MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.29M/7.29M [00:01<00:00, 5.08MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 24.7M/24.7M [00:04<00:00, 5.41MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 87.4M/87.4M [00:16<00:00, 5.45MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 27.6M/27.6M [00:08<00:00, 3.41MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 46.0M/46.0M [00:08<00:00, 5.42MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.81M/3.81M [00:00<00:00, 4.47MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33.6M/33.6M [00:06<00:00, 5.35MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 8.03M/8.03M [00:01<00:00, 4.81MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 25.8M/25.8M [00:07<00:00, 3.44MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7.93M/7.93M [00:01<00:00, 5.12MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.15M/9.15M [00:01<00:00, 5.10MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 33.4M/33.4M [00:05<00:00, 5.63MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 81.8M/81.8M [00:14<00:00, 5.73MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 34.4M/34.4M [00:06<00:00, 5.66MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53.3M/53.3M [00:11<00:00, 4.73MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 4.32M/4.32M [00:01<00:00, 4.31MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 35.9M/35.9M [00:07<00:00, 4.56MB/s]\n",
      "Generating train split: 10175723 examples [02:07, 79590.08 examples/s]\n",
      "Generating validation split: 985958 examples [00:12, 79502.17 examples/s]\n",
      "Generating test split: 1008786 examples [00:12, 78720.61 examples/s]\n",
      "/Users/zebulongoriely/Documents/UniDocs/PHD/research/projects/CHILDES_processor/env/lib/python3.9/site-packages/datasets/load.py:1461: FutureWarning: The repository for cambridge-climb/BabyLM contains custom code which must be executed to correctly load the dataset. You can inspect the repository content at https://hf.co/datasets/cambridge-climb/BabyLM\n",
      "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
      "Passing `trust_remote_code=True` will be mandatory to load this dataset from the next major release of `datasets`.\n",
      "  warnings.warn(\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10.4M/10.4M [00:01<00:00, 6.51MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 23.8M/23.8M [00:03<00:00, 6.98MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 13.6M/13.6M [00:02<00:00, 6.65MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 9.49M/9.49M [00:01<00:00, 7.30MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 28.7M/28.7M [00:03<00:00, 7.41MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 90.7M/90.7M [00:11<00:00, 7.63MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29.8M/29.8M [00:03<00:00, 7.56MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44.5M/44.5M [00:05<00:00, 7.87MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3.40M/3.40M [00:00<00:00, 5.03MB/s]\n",
      "Downloading data: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 29.1M/29.1M [00:06<00:00, 4.26MB/s]\n",
      "Generating train split: 1015485 examples [00:12, 79768.26 examples/s]\n",
      "Generating validation split: 985958 examples [00:12, 78678.63 examples/s]\n",
      "Generating test split: 1008786 examples [00:12, 78321.49 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "import pandas as pd\n",
    "\n",
    "dataset_strict = datasets.load_dataset(\"cambridge-climb/BabyLM\", \"original_strict\")\n",
    "dataset_strict_small = datasets.load_dataset(\"cambridge-climb/BabyLM\", \"original_strict_small\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'tagged_text', 'filename'],\n",
       "    num_rows: 1015485\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_strict_small['train']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning\n",
    "\n",
    "We apply some light cleaning to the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§¼ Cleaning 'strict_train'\n",
      "ðŸ§¹ Cleaned 'aochildes.txt' (size 763988 -> 763988)\n",
      "ðŸ§¹ Cleaned 'bnc_spoken.txt' (size 848199 -> 848199)\n",
      "ðŸ§¹ Cleaned 'cbt.txt' (size 263518 -> 263518)\n",
      "ðŸ§¹ Cleaned 'children_stories.txt' (size 76379 -> 76379)\n",
      "ðŸ§¹ Cleaned 'gutenberg.txt' (size 898292 -> 898292)\n",
      "ðŸ§¹ Cleaned 'open_subtitles.txt' (size 5433930 -> 5433127)\n",
      "ðŸ§¹ Cleaned 'qed.txt' (size 959619 -> 959844)\n",
      "ðŸ§¹ Cleaned 'simple_wikipedia.txt' (size 567001 -> 567001)\n",
      "ðŸ§¹ Cleaned 'switchboard.txt' (size 161739 -> 161739)\n",
      "ðŸ§¹ Cleaned 'wikipedia.txt' (size 203011 -> 177085)\n",
      "ðŸ§¼ Cleaning 'strict_small_train'\n",
      "ðŸ§¹ Cleaned 'aochildes.txt' (size 79999 -> 79999)\n",
      "ðŸ§¹ Cleaned 'bnc_spoken.txt' (size 89931 -> 89931)\n",
      "ðŸ§¹ Cleaned 'cbt.txt' (size 25999 -> 25999)\n",
      "ðŸ§¹ Cleaned 'children_stories.txt' (size 5731 -> 5731)\n",
      "ðŸ§¹ Cleaned 'gutenberg.txt' (size 94502 -> 94502)\n",
      "ðŸ§¹ Cleaned 'open_subtitles.txt' (size 527394 -> 527316)\n",
      "ðŸ§¹ Cleaned 'qed.txt' (size 99928 -> 99932)\n",
      "ðŸ§¹ Cleaned 'simple_wikipedia.txt' (size 56616 -> 56616)\n",
      "ðŸ§¹ Cleaned 'switchboard.txt' (size 15739 -> 15739)\n",
      "ðŸ§¹ Cleaned 'wikipedia.txt' (size 19615 -> 16930)\n",
      "ðŸ§¼ Cleaning 'strict_valid'\n",
      "ðŸ§¹ Cleaned 'aochildes.txt' (size 69999 -> 69999)\n",
      "ðŸ§¹ Cleaned 'bnc_spoken.txt' (size 89917 -> 89917)\n",
      "ðŸ§¹ Cleaned 'cbt.txt' (size 12746 -> 12746)\n",
      "ðŸ§¹ Cleaned 'children_stories.txt' (size 5989 -> 5989)\n",
      "ðŸ§¹ Cleaned 'gutenberg.txt' (size 80468 -> 80468)\n",
      "ðŸ§¹ Cleaned 'open_subtitles.txt' (size 529408 -> 529338)\n",
      "ðŸ§¹ Cleaned 'qed.txt' (size 94975 -> 94976)\n",
      "ðŸ§¹ Cleaned 'simple_wikipedia.txt' (size 60976 -> 60976)\n",
      "ðŸ§¹ Cleaned 'switchboard.txt' (size 17999 -> 17999)\n",
      "ðŸ§¹ Cleaned 'wikipedia.txt' (size 23477 -> 20236)\n",
      "ðŸ§¼ Cleaning 'strict_small_valid'\n",
      "ðŸ§¹ Cleaned 'aochildes.txt' (size 69999 -> 69999)\n",
      "ðŸ§¹ Cleaned 'bnc_spoken.txt' (size 89917 -> 89917)\n",
      "ðŸ§¹ Cleaned 'cbt.txt' (size 12746 -> 12746)\n",
      "ðŸ§¹ Cleaned 'children_stories.txt' (size 5989 -> 5989)\n",
      "ðŸ§¹ Cleaned 'gutenberg.txt' (size 80468 -> 80468)\n",
      "ðŸ§¹ Cleaned 'open_subtitles.txt' (size 529408 -> 529338)\n",
      "ðŸ§¹ Cleaned 'qed.txt' (size 94975 -> 94976)\n",
      "ðŸ§¹ Cleaned 'simple_wikipedia.txt' (size 60976 -> 60976)\n",
      "ðŸ§¹ Cleaned 'switchboard.txt' (size 17999 -> 17999)\n",
      "ðŸ§¹ Cleaned 'wikipedia.txt' (size 23477 -> 20236)\n"
     ]
    }
   ],
   "source": [
    "from cleaning import *\n",
    "\n",
    "CLEANUP_FUNCTIONS = {\n",
    "    'aochildes': cleanup_aochildes,\n",
    "    'bnc_spoken': cleanup_bnc_spoken,\n",
    "    'cbt': cleanup_cbt,\n",
    "    'children_stories': cleanup_children_stories,\n",
    "    'gutenberg': cleanup_gutenberg,\n",
    "    'open_subtitles': cleanup_open_subtitles,\n",
    "    'qed': cleanup_qed,\n",
    "    'simple_wikipedia': cleanup_simple_wikipedia,\n",
    "    'switchboard': cleanup_switchboard,\n",
    "    'wikipedia': cleanup_wikipedia,\n",
    "}\n",
    "\n",
    "def dataset_to_dataframe(dataset):\n",
    "    df = dataset.to_pandas()\n",
    "    df = df.drop(columns=['tagged_text'])\n",
    "    remove = ['None', 'nan', 'NaN']\n",
    "    # When saving the dataset, strings with 'None' or 'nan' or 'NaN'\n",
    "    # are converted to None values and this causes problems\n",
    "    df = df[~df['text'].isin(remove)]\n",
    "    return df\n",
    "\n",
    "def cleanup(df):\n",
    "    new_df = {'filename': [], 'text': []}\n",
    "    for filename in df['filename'].unique():\n",
    "        lines = list(df[df['filename'] == filename]['text'])\n",
    "        new_lines = CLEANUP_FUNCTIONS[filename.split('.')[0]]('\\n'.join(lines)).split('\\n')\n",
    "        new_lines = [new_line for new_line in new_lines if new_line.strip() != '']\n",
    "        new_df['filename'].extend([filename] * len(new_lines))\n",
    "        new_df['text'].extend(new_lines)\n",
    "        print(f\"ðŸ§¹ Cleaned '{filename}' (size {len(lines)} -> {len(new_lines)})\")\n",
    "    return pd.DataFrame(new_df)\n",
    "\n",
    "dfs = {\n",
    "    'strict_train': dataset_to_dataframe(dataset_strict['train']),\n",
    "    'strict_small_train': dataset_to_dataframe(dataset_strict_small['train']),\n",
    "    'strict_valid': dataset_to_dataframe(dataset_strict['validation']),\n",
    "    'strict_small_valid': dataset_to_dataframe(dataset_strict_small['validation']),\n",
    "}\n",
    "\n",
    "for name, df in dfs.items():\n",
    "    print(f\"ðŸ§¼ Cleaning '{name}'\")\n",
    "    dfs[name] = cleanup(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Phonemicize\n",
    "\n",
    "Use our phonemicizer code to add a phonemic transcription of every line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”  Adding phonemes to 'strict_train'\n",
      "Phonemizing using language \"EnglishNA\"...\n",
      "Using espeak backend with language code \"en-us\"...\n",
      "ðŸ”  Added phonemes... (size 10149172 -> 10117701)\n",
      "ðŸ”  Adding phonemes to 'strict_small_train'\n",
      "Phonemizing using language \"EnglishNA\"...\n",
      "Using espeak backend with language code \"en-us\"...\n",
      "ðŸ”  Added phonemes... (size 1012695 -> 1009906)\n",
      "ðŸ”  Adding phonemes to 'strict_valid'\n",
      "Phonemizing using language \"EnglishNA\"...\n",
      "Using espeak backend with language code \"en-us\"...\n",
      "ðŸ”  Added phonemes... (size 982644 -> 979449)\n",
      "ðŸ”  Adding phonemes to 'strict_small_valid'\n",
      "Phonemizing using language \"EnglishNA\"...\n",
      "Using espeak backend with language code \"en-us\"...\n",
      "ðŸ”  Added phonemes... (size 982644 -> 979449)\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "os.environ['PHONEMIZER_ESPEAK_LIBRARY'] = '/opt/local/lib/libespeak-ng.dylib'\n",
    "sys.path.append('../../')\n",
    "from src.phonemize import phonemize_utterances, character_split_utterance\n",
    "\n",
    "def add_phonemes(df):\n",
    "    lines = df['text'].tolist()\n",
    "    len_before = len(lines)\n",
    "    # The Espeak backend is used for phonemization but will sometimes place word boundaries in places\n",
    "    # that don't match the orthography. E.g. \"that's it\" might become one word instead of two. This is\n",
    "    # not so much a problem for our cases, unless we're interested in the word boundaries themselves.\n",
    "    phonemized = phonemize_utterances(lines, keep_word_boundaries=True, allow_possibly_faulty_word_boundaries=True)\n",
    "    df['phonemized_utterance'] = phonemized\n",
    "    remove = ['None', 'nan', 'NaN', '', ' ', '  ', None]\n",
    "    # We also split the phonemized text into characters for the character-level model\n",
    "    df['character_split_utterance'] = character_split_utterance(lines)\n",
    "    df = df[~df['phonemized_utterance'].isin(remove)]\n",
    "    len_after = len(df)\n",
    "    print(f\"ðŸ”  Added phonemes... (size {len_before} -> {len_after})\")\n",
    "    return df\n",
    "\n",
    "for name, df in dfs.items():\n",
    "    print(f\"ðŸ”  Adding phonemes to '{name}'\")\n",
    "    dfs[name] = add_phonemes(df) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save to Huggingface Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, df in dfs.items():\n",
    "    # We don't need the entire validation set\n",
    "    if 'valid' in name:\n",
    "        df = df.sample(n=10000, random_state=42)\n",
    "    df.to_csv(f'BabyLM-phonemized/{name}.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test import of dataset\n",
    "\n",
    "The dataset is saved at `BabyLM-phonemized/`. We don't need to push it to Huggingface to load it here, we can provide a local path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 10117701 examples [01:34, 107153.37 examples/s]\n",
      "Generating valid split: 10000 examples [00:00, 141522.56 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "train_dataset = load_dataset('BabyLM-phonemized', 'strict', split='train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Good.\n"
     ]
    }
   ],
   "source": [
    "print(train_dataset['text'][764123])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, line in enumerate(train_dataset['phonemized_utterance']):\n",
    "    if line is None:\n",
    "        print('None found')\n",
    "        print(i)\n",
    "        print(line)\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
